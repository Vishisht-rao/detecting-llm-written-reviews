Write a review for the following paper.

Abstract: In a controlled experiment of sequence-tosequence approaches for the task of sentence correction, we find that characterbased models are generally more effective than word-based models and models that encode subword information via convolutions, and that modeling the output data as a series of diffs improves effectiveness over standard approaches. Our strongest sequence-to-sequence model improves over our strongest phrase-based statistical machine translation model, with access to the same data, by 6 M2 (0.5 GLEU) points. Additionally, in the data environment of the standard CoNLL-2014 setup, we demonstrate that modeling (and tuning against) diffs yields similar or better M2 scores with simpler models and/or significantly less data than previous sequence-to-sequence approaches.

1 Introduction: The task of sentence correction is to convert a natural language sentence that may or may not have errors into a corrected version. The task is envisioned as a component of a learning tool or writing-assistant, and has seen increased interest since 2011 driven by a series of shared tasks (Dale and Kilgarriff, 2011; Dale et al., 2012; Ng et al., 2013, 2014).
Most recent work on language correction has focused on the data provided by the CoNLL-2014 shared task (Ng et al., 2014), a set of corrected essays by second-language learners. The CoNLL2014 data consists of only around 60,000 sentences, and as such, competitive systems have made use of large amounts of corrected text without annotations, and in some cases lower-quality
crowd-annotated data, in addition to the shared data. In this data environment, it has been suggested that statistical phrase-based machine translation (MT) with task-specific features is the state-of-the-art for the task (Junczys-Dowmunt and Grundkiewicz, 2016), outperforming wordand character-based sequence-to-sequence models (Yuan and Briscoe, 2016; Xie et al., 2016; Ji et al., 2017), phrase-based systems with neural features (Chollampatt et al., 2016b,a), re-ranking output from phrase-based systems (Hoang et al., 2016), and combining phrase-based systems with classifiers trained for hand-picked subsets of errors (Rozovskaya and Roth, 2016).
We revisit the comparison across translation approaches for the correction task in light of the Automated Evaluation of Scientific Writing (AESW) 2016 dataset, a correction dataset containing over 1 million sentences, holding constant the training data across approaches. The dataset was previously proposed for the distinct binary classification task of grammatical error identification.
Experiments demonstrate that pure characterlevel sequence-to-sequence models are more effective on AESW than word-based models and models that encode subword information via convolutions over characters, and that representing the output data as a series of diffs significantly increases effectiveness on this task. Our strongest character-level model achieves statistically significant improvements over our strongest phrasebased statistical machine translation model by 6 M2 (0.5 GLEU) points, with additional gains when including domain information. Furthermore, in the partially crowd-sourced data environment of the standard CoNLL-2014 setup in which there are comparatively few professionally annotated sentences, we find that tuning against the tags marking the diffs yields similar or superior effectiveness relative to existing sequencear X iv :1
70 7.
09 06
7v 1
[ cs
.C L
] 2
7 Ju
l 2 01
7
to-sequence approaches despite using significantly less data, with or without using secondary models. All code is available at https://github. com/allenschmaltz/grammar.

2 Background and Methods: Task We follow recent work and treat the task of sentence correction as translation from a source sentence (the unedited sentence) into a target sentence (a corrected version in the same language as the source). We do not make a distinction between grammatical and stylistic corrections.
We assume a vocabulary V of natural language word types (some of which have orthographic errors). Given a sentence s = [s1 · · · sI ], where si ∈ V is the i-th token of the sentence of length I , we seek to predict the corrected target sentence t = [t1 · · · tJ ], where tj ∈ V is the j-th token of the corrected sentence of length J . We are given both s and t for supervised training in the standard setup. At test time, we are only given access to sequence s. We learn to predict sequence t (which is often identical to s).
Sequence-to-sequence We explore word and character variants of the sequence-to-sequence framework. We use a standard word-based model (WORD), similar to that of Luong et al. (2015), as well as a model that uses a convolutional neural network (CNN) and a highway network over characters (CHARCNN), based on the work of Kim et al. (2016), instead of word embeddings as the input to the encoder and decoder. With both of these models, predictions are made at the word level. We also consider the use of bidirectional versions of these encoders (+BI).
Our character-based model (CHAR+BI) follows the architecture of the WORD+BI model, but the input and output consist of characters rather than words. In this case, the input and output sequences are converted to a series of characters and whitespace delimiters. The output sequence is converted back to t prior to evaluation.
The WORD models encode and decode over a closed vocabulary (of the 50k most frequent words); the CHARCNN models encode over an open vocabulary and decode over a closed vocabulary; and the CHAR models encode and decode over an open vocabulary.
Our contribution is to investigate the impact of sequence-to-sequence approaches (including those not considered in previous work) in a series
of controlled experiments, holding the data constant. In doing so, we demonstrate that on a large, professionally annotated dataset, the most effective sequence-to-sequence approach can significantly outperform a state-of-the-art SMT system without augmenting the sequence-to-sequence model with a secondary model to handle lowfrequency words (Yuan and Briscoe, 2016) or an additional model to improve precision or intersecting a large language model (Xie et al., 2016). We also demonstrate improvements over these previous sequence-to-sequence approaches on the CoNLL-2014 data and competitive results with Ji et al. (2017), despite using significantly less data.
The work of Schmaltz et al. (2016) applies WORD and CHARCNN models to the distinct binary classification task of error identification.
Additional Approaches The standard formulation of the correction task is to model the output sequence as t above. Here, we also propose modeling the diffs between s and t. The diffs are provided in-line within t and are described via tags marking the starts and ends of insertions and deletions, with replacements represented as deletioninsertion pairs, as in the following example selected from the training set: “Some key points are worth <del> emphasiz </del><ins> emphasizing </ins> .”. Here, “emphasiz” is replaced with “emphasizing”. The models, including the CHAR model, treat each tag as a single, atomic token.
The diffs enable a means of tuning the model’s propensity to generate corrections by modifying the probabilities generated by the decoder for the 4 diff tags, which we examine with the CoNLL data. We include four bias parameters associated with each diff tag, and run a grid search between 0 and 1.0 to set their values based on the tuning set.
It is possible for models with diffs to output invalid target sequences (for example, inserting a word without using a diff tag). To fix this, a deterministic post-processing step is performed (greedily from left to right) that returns to source any non-source tokens outside of insertion tags. Diffs are removed prior to evaluation. We indicate models that do not incorporate target diff annotation tags with the designator –DIFFS.
The AESW dataset provides the paragraph context and a journal domain (a classification of the document into one of nine subject categories) for each sentence.1 For the sequence-to-sequence
1The paragraphs are shuffled for purposes of obfuscation,
models we propose modeling the input and output sequences with a special initial token