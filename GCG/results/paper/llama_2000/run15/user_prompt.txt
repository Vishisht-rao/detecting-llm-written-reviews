Write a review for the following paper.

Abstract: In [1], we introduced mechanical learning and proposed 2 approaches to mechanical learning. Here, we follow one such approach to well describe the objects and the processes of learning. We discuss 2 kinds of patterns: objective and subjective pattern. Subjective pattern is crucial for learning machine. We prove that for any objective pattern we can find a proper subjective pattern based upon least base patterns to express the objective pattern well. X-form is algebraic expression for subjective pattern. Collection of X-forms form internal representation space, which is center of learning machine. We discuss learning by teaching and without teaching. We define data sufficiency by X-form. We then discussed some learning strategies. We show, in each strategy, with sufficient data, and with certain capabilities, learning machine indeed can learn any pattern (universal learning machine). In appendix, with knowledge of learning machine, we try to view deep learning from a different angle, i.e. its internal representation space and its learning dynamics.

1 Introduction: Mechanical learning is a computing system that is based on a simple set of fixed rules (so called mechanical), and can modify itself according to incoming data (so called learning). A learning machine M is a system that realizes mechanical learning.
∗Great thanks for whole heart support of my wife. Thanks for Internet and research contents contributers to Internet.
1
ar X
iv :1
70 6.
00 06
6v 1
[ cs
.A I]
3 1
M ay
In [1], we introduced mechanical learning and discussed some basic aspects of it. Here, we are going to continue the discussion of mechanical learning. As we proposed in [1], there are naturally 2 ways to go: to directly realize one learning machine, or to well describe what mechanical learning is really doing. Here, we do not try to design a specific learning machine, instead, we focus on describing the mechanical learning, specially, the objects and the process of learning, and related properties. Again, the most important assumption is mechanical, i.e., the system must follow a set of simple and fixed rules. By posting such requirement on learning, we can go deeper and reveal more interesting properties of learning.
In section 2, we discuss more about learning machine. We show one useful simplification: a N -M learning machine can be reduced to M independent N -1 learning machines. This simplification could help us a lot. We define level 1 learning machine in section 2. This concept clarifies a lot of confusing.
The driving force of a learning machine M is its incoming data, and incoming data forms patterns. Thus, we need to understand pattern first. In section 3, we discuss patterns and examples. In the process of understanding pattern, what is objective and what is subjective is naturally raised. In fact, these issues are very crucial to learning machine. Objective patterns and their basic operators are straightforward. In order to understand subjective pattern, we discuss how learning machine to perceive and process pattern. Such discussions lead us to subjective pattern and basic operators on them. We introduce X-form for subjective expressions, which will play central role in our later discussions. We prove that for any objective pattern we can find a proper X-form based upon least base patterns and to express the objective pattern well.
Learning by teaching, i.e. learning driving by a well designed teaching sequence (a special kind of data sequence), is a much simpler and effective learning. Though learning by teaching is only available in very rare cases, it is very educational to discuss it first. This is what we do in section 4. We show if a learning machine has certain capabilities, we can make teaching sequence so that under driven of such teaching sequence, it learns effectively. So, with these capabilities, we have an universal learning machine.
From learning by teaching, we get insight that the most crucial part of learning is abstraction from lower to higher. We try to apply such insights to learning without teaching. In section 5, we first defined mechanical learning without teaching. Then we introduce internal representation space, which is the center of learning machine and best to be expressed by X-forms. Internal representation space is actually where learning is happening. We write down the formulation of learning dynamics, which gives a clear picture about how data drives learning. However, one big issue is how much data are enough to drive the learning to reach the target. With the help of X-form and sub-form, we define data sufficiency: sufficient to support a X-form, and sufficient to bound a X-form. Such sufficiency gives a framework for us to understand data used to drive learning. We then show that by a proper learning strategy, with sufficient data, with certain learning capabilities, a learning machine indeed can learn. We demonstrate 3 learning strategies: embed into parameter space, squeezing to higher abstraction from inside, and squeezing to higher abstraction from inside and outside. We show that the first learning strategy is actually what deep learning is using (see Appendix for details). And, we show that by other 2 learning strategies with certain learning capabilities, a learning machine can learn any pattern, i.e. it is an universal learning machine. Squeezing to higher abstraction and more generalization is one strategy that we invent here. We believe that this strategy would work well for many learning tasks. We need to do more works in this direction.
In Section 6, we put more thoughts about learning machine. We will continue work on these directions. In section 7, we briefly discuss some issues of designing a learning machine.
In Appendix, we view deep learning (restricted to the stacks of RBMs) from our point of view, i.e. internal representation space. We start discussions from simplest, i.e. 2-1 RBM, then 3-1 RBM, N-1 RBM, N-M RBM, and stacks of RBM, and deep learning. In this way, it is clear that deep learning is using the learning strategy: embed a group of X-forms into parameter space that we discuss in section 5.
As in [1], for the same reason, here we will restrict to spatial learning, not consider it temporal learning.

2 Learning Machine: 

IPU – Information Processing Unit: We have discussed mechanical learning in [1]. A learning machine is a concrete realization of mechanical learning. We can briefly recall them here. See the illustration of IPU (Information Processing Unit):
Fig. 1. Illustration of N-M IPU (Information Processing Unit)
One N -M IPU has input space (N bits) and output space (M bits), and it will process input to output. If the processing are adapting according to input and feedback to output, and such adapting is governed by a set of simple and fixed rules, we call such adapting as mechanical learning, and such IPU as learning machine. Notice the phrase ”a set of simple and fixed rules”. This is a strong restriction. Mostly, we use this phrase to rule out human intervention. And, we pointed out this: since the set of adapting rules is fixed, we can reasonablly think the adapting rules are built inside learning machine at the setup.
We will try to well describe learning machine. First, we can put one simple observation here.
Theorem 2.1 One N -M IPU M is equivalent to M N -1 IPU Mi, i = 1, . . . ,M .
Proof: The output space of M is M -dim, so, we assume it is (v1, v2, . . . , vM ). If we project to first component, i.e. v1, we get a N -1 IPU, denote it as: M1. We can do same for vi, i = 2, . . . ,M , and get N -1 IPUs: M2, . . . ,MM . This tells us, if we have one N -M IPU M, we can get M N -1 IPU M1, . . ., so that M = (M1,M2, . . .MM ).
On the other side, if we have M N -1 IPU, M1,M2, . . . ,MM , we can use them to form a N -M IPU in this way: M = (M1,M2, . . .MM ).
Though this theorem is very simple, it can make our discussion much simpler. For most time, we can only consider N -1 IPU, which is much simpler to discuss. However, this is only to consider IPU, i.e. ability to process information. For learning, we need to consider more. See theorem 2.
The purpose or target of learning machine: One learning machine is one IPU, i.e. it will do information processing for each input and generate output, it maps one input (a N -dim binary vector) to a M -dim binary vector. This is what a CPU does as well (More abstractly, since we do not restrict the size of N and M , any software without temporal effect can be thought as one IPU).
