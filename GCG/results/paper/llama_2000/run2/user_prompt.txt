Write a review for the following paper.

Abstract: A key characteristic of work on deep learning and neural networks in general is that it relies on representations of the input that support generalization, robust inference, domain adaptation and other desirable functionalities. Much recent progress in the field has focused on efficient and effective methods for computing representations. In this paper, we propose an alternative method that is more efficient than prior work and produces representations that have a property we call focality – a property we hypothesize to be important for neural network representations. The method consists of a simple application of two consecutive SVDs and is inspired by (Anandkumar et al., 2012). In this paper, we propose to generate representations for deep learning by two consecutive applications of singular value decomposition (SVD). In a setup inspired by (Anandkumar et al., 2012), the first SVD is intended for denoising. The second SVD rotates the representation to increase what we call focality. In this initial study, we do not evaluate the representations in an application. Instead we employ diagnostic measures that may be useful in their own right to evaluate the quality of representations independent of an application. We use the following terminology. SVD (resp. SVD) refers to the method using one (resp. two) applications of SVD; 1LAYER (resp. 2LAYER) corresponds to a single-hidden-layer (resp. twohidden-layer) architecture. In Section 1, we introduce the two methods SVD and SVD and show that SVD generates better (in a sense to be defined below) representations than SVD. In Section 2, we compare 1LAYER and 2LAYER SVD representations and show that 2LAYER representations are better. Section 3 discusses the results. We present our conclusions in Section 4.

2 1LAYER vs. 2LAYER: We compare two representations of a word trigram: (i) the 1LAYER representation from Section 1 and (ii) a 2LAYER representation that goes through two rounds of autoencoding, which is a deep learning representation in the sense that layer 2 represents more general and higher-level properties of the input than layer 1.
The architecture of the 2LAYER is depicted in Figure 3.
To create 2LAYER representations, we first create a vector for each of the 20701 word types occurring in the corpus. This vector is the concatenation of its left vector and its right vector. The resulting 20701 × 500 matrix is the input representation to SVD1. We again set k = 100. A trigram
is then represented as the concatenation of three of these 100-dimensional vectors. We apply the SVD2 construction algorithm to the resulting 100000 × 300 matrix and truncate to k = 100.
We now have – for each trigram – two SVD2 representations, the 1LAYER representation from Section 1 and the 2LAYER representation we just described. We compare these two trigram representations, again using the task from Section 1: discrimination of the 100 pairs of words.
2LAYER is better than 1LAYER 64 times on this task, the same in 18 cases and worse in 18 cases. This is statistically significant (p < .01, binomial test) evidence that 2LAYER SVD2 representations are more focal than 1LAYER SVD2 representations.

3 Discussion: 

3.1 Focality: One advantage of focal representations is that many classifiers cannot handle conjunctions of several features unless they are explicitly defined as separate features. Compare two representations ~x and ~x′ where ~x′ is a rotation of ~x (as it might be obtained by an SVD). Since one vector is a rotation of the other, they contain exactly the same information. However, if (i) an individual “hidden unit” of the rotated vector ~x′ can directly be interpreted as “is verb” (or a similar property like “is adjective” or “takes NP argument”) and (ii) the same feature requires a conjunction of several hidden units for ~x, then the rotated representation is superior for many upstream statistical classifiers.
Focal representations can be argued to be closer to biological reality than broadly distributed representations (Thorpe, 2010); and they have the nice property that they become categorical in the limit. Thus, they include categorical representations as a special case.
A final advantage of focal representations is that in some convolutional architectures the input to the top-layer statistical classifier consists of maxima over HU (hidden unit) activations. E.g., one way to classify a sentence as having positive/negative sentiment is to slide a neural network whose input is a window of k words (e.g., k = 4) over it and to represent each window of k words as a vector of HU activations produced by the network. In a focal representation, the hidden units are more likely to have clear semantics like “the window contains a positive sentiment word”. In this type of scenario, taking the maximum of activations over the n − k + 1 sliding windows of a sentence of length n results in hidden units with interpretable semantics like “the activation of the positive-sentiment HU of the window with the highest activation for this HU”. These maximum values are then a good basis for sentiment classification of the sentence as a whole.
The notion of focality is similar to disentanglement (Glorot et al., 2011) – in fact, the two notions may be identical. However, Glorot et al. (2011) introduce disentanglement in the context of domain adaptation, focusing on the idea that “disentangled” hidden units capture general cross-domain properties and for that reason are a good basis for domain adaptation. The contributions of this paper are: proposing a way of measuring “entanglement” (i.e., measuring it as correlation), defining focality
in terms of classification accuracy (a definition that covers single hidden units as well as groups of hidden units) and discussing the relationship to convolution and biological systems.
It is important to point out that we have not addressed how focality would be computed efficiently in a particular context. In theory, we could use brute force methods, but these would be exponential in the number of dimensions (systematic search over all subsets of dimensions). However, certain interesting questions about focality can be answered efficiently; e.g., if we have M(f,R) = 1 for one representation and M(f,R′) > 1 for another, then this can be shown efficiently and in this case we have established that R is more focal than R′.

3.2 mSVD method: In this section, we will use the abbreviation mSVD to refer to a stacked applications of our method with an arbitrary number of layers even though we only experiment with m = 2 in this paper (2LAYER, 2-layer-stacking).
SVD and other least squares methods are probably the most widely used dimensionality reduction techniques for the type of matrices in natural language processing that we work with in this paper (cf. (Turney and Pantel, 2010)). Stacking a second least squares method on top of the first has not been considered widely because these types of representations are usually used directly in vector classifiers such as Rocchio and SVM (however, see the discussion of (Chen et al., 2012) below). For this type of classifier, performing a rotation has no effect on classification performance. In contrast, our interest is to use SVD2 representations as part of a multilevel deep learning architecture where the hidden unit representations of any given layer are not simply interpreted as a vector, but decisions of higher layers can be based on individual dimensions.
The potential drawback of SVD and other least squares dimensionality reductions is that they are linear: reduced dimensions are linear combinations of orginal dimensions. To overcome this limitation many nonlinear methods have been introduced: probabilistic latent semantic indexing (Hofmann, 1999), kernel principal component analysis (Schölkopf et al., 1998), matrix factorization techniques that obey additional constraints – such as non-negativity in the case of non-negative matrix factorization (Lee and Seung, 1999) – , latent dirichlet allocation (Blei et al., 2003) and different forms of autoencoding (Bengio, 2009; Chen et al., 2012). All of these can be viewed as dimension reduction techniques that do not make the simplistic assumptions of SVD and should therefore be able to produce better representation if these simplistic assumptions are not appropriate for the domain in question.
However, this argument does not apply to the mSVD method we propose in this paper since it is also nonlinear. What should be investigated in the future is to