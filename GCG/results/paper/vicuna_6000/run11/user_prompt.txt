Write a review for the following paper.

Abstract: Feature selection, as a data preprocessing strategy, has been proven to be effective and efficient in preparing high-dimensional data for data mining and machine learning problems. The objectives of feature selection include: building simpler and more comprehensible models, improving data mining performance, and preparing clean, understandable data. The recent proliferation of big data has presented some substantial challenges and opportunities of feature selection algorithms. In this survey, we provide a comprehensive and structured overview of recent advances in feature selection research. Motivated by current challenges and opportunities in the big data age, we revisit feature selection research from a data perspective, and review representative feature selection algorithms for generic data, structured data, heterogeneous data and streaming data. Methodologically, to emphasize the differences and similarities of most existing feature selection algorithms for generic data, we generally categorize them into four groups: similarity based, information theoretical based, sparse learning based and statistical based methods. Finally, to facilitate and promote the research in this community, we also present a open-source feature selection repository that consists of most of the popular feature selection algorithms (http://featureselection.asu.edu/scikit-feast/). At the end of this survey, we also have a discussion about some open problems and challenges that need to be paid more attention in future research.

1. Introduction: We are now in the era of big data, where massive amounts of high dimensional data has become ubiquitous in our daily life, such as social media, e-commerce, health care, bioinformatics, transportation, online education, etc. Figure (1) shows an example by plotting the growth trend of UCI machine learning repository (Bache and Lichman, 2013). Rapid growth of data presents challenges for effective and efficient data management. Therefore, it is desirable and of great importance to apply data mining and machine learning techniques to automatically discover knowledge from these data.
When applying data mining and machine learning algorithms on high dimensional data, a critical issue is known as curse of dimensionality (Hastie et al., 2005). It refers to the phenomenon that data becomes sparser in high dimensional space, adversely affecting algorithms designed for low dimensional space. In addition, with the existence of a large number of features, learning models tend to overfit which may cause performance degradation on unseen data. Moreover, data of high dimension significantly increases the memory storage requirements and computational costs for data analytics.
Dimensionality reduction is one of the most powerful tools to address the previously described issues. It can be categorized mainly into into two main components: feature extraction and feature selection. Feature extraction projects original high dimensional feature space to a new feature space with low dimensionality. The new constructed feature space is usually a linear or nonlinear combination of the original feature space. Examples of feature extraction methods include Principle Component Analysis (PCA) (Jolliffe, 2002), Linear Discriminant Analysis (LDA) (Scholkopft and Mullert, 1999), Canonical Correlation Analysis (CCA) (Hardoon et al., 2004), Singular Value Decomposition (Golub and Van Loan, 2012), ISOMAP (Tenenbaum et al., 2000) and Locally Linear Embedding (LLE) (Roweis and Saul, 2000). Feature selection, on the other hand, directly selects a subset of relevant features for the use model construction. Lasso (Tibshirani, 1996), Information Gain (Cover and Thomas, 2012), Relief (Kira and Rendell, 1992a), MRMR (Peng et al., 2005), Fisher Score (Duda et al., 2012), Laplacian Score (He et al., 2005), and SPEC (Zhao and Liu, 2007) are some of the well known feature selection techniques.
Both feature extraction and feature selection have the advantage of improving learning performance, increasing computational efficiency, decreasing memory storage requirements, and building better generalization models. However, since feature extraction builds a set of new features, further analysis is problematic as we cannot get the physical meaning of these features in the transformed space. In contrast, by keeping some original features, feature selection maintains physical meanings of original features, and gives models better readability and interpretability. Therefore, feature selection is often preferred in many realworld applications such as text mining and genetic analysis compared to feature extraction.
Real-world data is usually imperfect, containing some irrelevant and redundant features. Removing these features by feature selection reduces storage and computational cost while avoiding significant loss of information or negative degradation of learning performance. For example, in Figure (2(a)), feature f1 is a relevant feature which is able to discriminate two classes (clusters). However, given feature f1, feature f2 in Figure (2(b)) is redundant as f2 is strongly correlated with f1. In Figure (2(c)), feature f3 is an irrelevant feature as it cannot separate two classes (clusters) at all. Therefore, the removal of f2 and f3 will not negatively impact the learning performance.
In the following subsections, we first review traditional categorizations of feature selection algorithms from the availability of labels and from the search strategy perspectives in Section 1.1. In Section 1.2, we revisit feature selection from a data perspective motivated by challenges and opportunities from big data. Meanwhile, we discuss the necessity for a comprehensive and structured overview of current advances on feature selection, and our efforts to build a open source machine learning repository to cover state-of-the-art feature selection algorithms. In Section 1.3, we give an outline and organization of the survey.

1.1 Traditional Categorizations of Feature Selection Algorithms: 

1.1.1 Label Perspective: According to the availability of label information, feature selection algorithms can be broadly classified as supervised, unsupervised and semi-supervised methods.
Supervised Feature Selection Supervised feature selection is generally designed for classification or regression problems. It aims to select a subset of features that are able to discriminate samples from different classes. With the existence of class labels, the feature relevance is usually assessed via its correlation with class labels. A general framework of supervised feature selection is illustrated in Figure (3). The training phase of the classification highly depends on feature selection. After splitting the data into training and testing sets, classifiers are trained based on a subset of features selected by supervised feature selection. Note that the feature selection phase can either be independent of the learning algorithm (filter methods), or it may iteratively take advantage of the learning performance of a classifier to assess the quality of selected features so far (wrapper methods). Finally, the trained classifier predicts class labels of samples in the test set on the selected features.
Unsupervised Feature Selection Unsupervised feature selection is generally designed for clustering problems. Since acquiring labeled data is particularly expensive in both time and effort, unsupervised feature selection on unlabeled data has recently gained considerable attention recently. Due to the lack of label information to evaluate the importance of features, unsupervised feature selection methods seek alternative criteria to define the relevance of features such as data similarity and local discriminative information. A general framework of unsupervised feature selection is illustrated in Figure (4). Different from supervised feature selection, unsupervised feature
selection usually uses all instances are available in the feature selection phase. The feature selection phase is either be independent of the unsupervised learning algorithms (filter methods), or it relies on the learning algorithms to iteratively improve the quality of selected features (wrapper methods). After the feature selection phase, it outputs the cluster structure of all data samples on the selected features by using a typical clustering algorithm.
Semi-Supervised Feature Selection Supervised feature selection works when sufficient label information is available while unsupervised feature selection algorithms do not require any label information. However, in many real-world applications, we usually have a small number of labeled samples and a large number of unlabeled samples. Both supervised and unsupervised feature selection algorithms cannot fully take advantage of all samples in this scenario. For supervised methods, the small number of labeled samples may be insufficient to provide correlation information of features; while unsupervised methods totally ignore class labels which could provide useful information to discriminate different classes. Therefore, it is desirable to develop semi-supervised methods by exploiting both labeled and unlabeled samples. We provide a general framework of semi-supervised feature selection in Figure (5), it is similar to the framework of supervised feature selection except that in semi-supervised methods only partial label information is available.

1.1.2 Search Strategy Perspective: With respect to different selection strategies, feature selection methods can be categorized as wrapper, filter and embedded methods.
Wrapper