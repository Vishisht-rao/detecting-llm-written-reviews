Write a review for the following paper.

Abstract: Mobile app distribution platforms such as Google Play Store allow users to share their feedback about downloaded apps in the form of a review comment and a corresponding star rating. Typically, the star rating ranges from one to ve stars, with one star denoting a high sense of dissatisfaction with the app and ve stars denoting a high sense of satisfaction. Unfortunately, due to a variety of reasons, oen the star rating provided by a user is inconsistent with the opinion expressed in the review. For example, consider the following review for the Facebook App on Android; “Awesome App”. One would reasonably expect the rating for this review to be ve stars, but the actual rating is one star! Such inconsistent ratings can lead to a deated (or inated) overall average rating of an app which can aect user downloads, as typically users look at the average star ratings while making a decision on downloading an app. Also, the app developers receive a biased feedback about the application that does not represent ground reality. is is especially signicant for small apps with a few thousand downloads as even a small number of mismatched reviews can bring down the average rating drastically. In this paper, we conducted a study on this review-rating mismatch problem. We manually examined 8600 reviews from 10 popular Android apps and found that 20% of the ratings in our dataset were inconsistent with the review. Further, we developed three systems; two of which were based on traditional machine learning and one on deep learning to automatically identify reviews whose rating did not match with the opinion expressed in the review. Our deep learning system performed the best and had an accuracy of 92% in identifying the correct star rating to be associated with a given review. In another evaluation, we asked 23 end users to write reviews for any 5 apps that they had used recently. We got 115 reviews from 66 dierent mobile apps. Our deep learning system had an accuracy of 87%. Further, our study suggests that this problem is quite prevalent among apps. Across the ten apps used in our study, the mismatch percentage ranged from 16% to 26%.

1 INTRODUCTION: Mobile apps are typically available for download at digital distribution platforms like Google Play Store and Apple Store. Once a user has downloaded and used an app, these distribution platforms also allow the user to enter feedback about the app. e feedback is received in the form of a review comment and an associated star rating. e star rating ranges from one to ve stars with one star denoting extreme dissatisfaction with an app and ve stars denoting high satisfaction.
e review comments and star ratings are very important as studies and our survey show that users typically download an app based on these factors [9].
As ratings are an important factor in determining the download of an app, it is imperative that ratings be accurate i.e., a rating accurately re ects the experience of the user with the app.
However, a study [3] and our investigations suggest that o en the star rating given by a user is not consistent with the opinion expressed in the review comment. Consider the following review text for the Instagram app on Android.
“Love instagram it’s the best in the world Love it it’s the best in the world” (sic)
One would reasonably expect that due to the highly positive sentiment expressed in the review, the associated star rating would be ve stars, but the actual star rating is one star! Such mismatches bring down the average rating of an app, which can adversely a ect future downloads of the app (especially for small and upcoming apps without many downloads).
Review rating mismatches can occur due to a variety of reasons; one reason could be that novice end users may simply be confused about the di erence between one and ve stars [3]. On the other end, a negative opinion accompanied by ve stars could happen due to the following reason: A user may initially provide a rating of ve stars to an app due to a positive experience. Review systems allow users to simply rate without an explicit review comment. Later on, the user may have a negative experience with the app, (usually a er an update). He may then write about his problems with the app, but may forget to update the rating to accurately re ect his current review text. us, a review with a largely negative opinion can have a high rating of ve stars. is hypothesis is in fact con rmed by our survey responses in Section 2
In this paper, we perform a study of this review rating mismatch problem. We rst establish by means of a user survey and manual study that the review rating mismatch problem is prevalent across
ar X
iv :1
70 8.
04 96
8v 1
[ cs
.L G
] 1
6 A
ug 2
01 7
popular apps on Android. We also establish the need for a system which can automatically detect inconsistencies between reviews and ratings. We then show that the development of such an automated system is non-trivial i.e., simple techniques such as natural language sentiment analysis do not su ce.
We then empirically establish that our system performs well i.e., can accurately identify reviews whose ratings do not match with the opinion expressed in the review text.
We use our automated system to nd the prevalence of reviewrating mismatches across 10 popular apps on Android and discover that 16% to 26% of the ratings do not match with their reviews. We nally show the generalizability of our system by analyzing mismatches on datasets of completely di erent domains.
To summarize, the main contributions of this paper are as follows:
• A survey of Android app end users and developers which suggests that: – Review text and star rating should match – It is useful to have an automated system to detect
mismatches – Users do not update their rating when they change
the review text • A manual investigation of 8600 reviews from 10 popular
apps on Android. is study shows that about 20% of the reviews have inconsistent ratings and this inconsistency is distributed across apps. • Machine learning and deep learning techniques to automatically detect mismatched review-ratings. • A deep learning model which achieves a cross-validation accuracy of 92% in identifying reviews with inconsistent ratings. • An evaluation with 23 independent human evaluators on a test set of 115 reviews drawn from 66 diverse mobile apps. e accuracy ranged from 84% to 87% • An estimate of the prevalence of review-rating mismatches across 10 popular Android apps using our deep learning system. e mismatch ranges from 16% to 26%
e remainder of this paper is organized as follows: Section 2 provides the motivation, Section 3 describes our approach to solving this review rating mismatch problem, Section 4 describes the evaluation of our approach. Section 5 discusses the implications of our work and Section 6 describes the related work (which has mainly focused on extracting feature requests and bugs from reviews and not on detecting inconsistent review-ratings). We conclude in Section 7.

2 MOTIVATION: In this section, we provide motivation for our work using two methods:
• A survey of Android app end users and developers • A manual annotation of reviews from popular Android
apps

2.1 Motivating Survey: In our survey, we primarily wanted to know whether users believed that a mobile app star rating and associated review text should
match, whether an automated system to detect mismatched reviews is useful and whether users update the star rating when they update the review text.
We hosted the survey questions on Google Forms and posted the link on di erent platforms such as Android forums, mailing lists, bulletin boards of the Computer Science Department at two premier universities in our country and organization.
No compensation was provided to any of the survey participants. ey were not told about our hypothesis about review-rating mismatch. e survey had two branches based on whether the respondent was an Android app developer or only an end user. e end users had seven questions while the developers had four questions.
We received 109 responses to our survey with 82% being end users and 18% developers. e survey responses are shown in Tables 1, 2, 3 and 4.
e fundamental premise of our work that the star rating and the associated review text should correspond is strongly supported by the responses shown in Table 1. Further, Table 2 suggests that both end-users and developers feel that an automated system to detect mismatched review-ratings is useful.
Table 3 shows the other survey questions to end users and their responses. It suggests that users base their download decision on existing reviews and average rating. us, if we have a number of ratings that are inconsistent with the review text (say, the correct rating should have been ve, but the user rated as one), the average rating of the app may decrease which will in turn will a ect app downloads. For widely popular