Write a review for the following paper.

Abstract: It has been believed that stochastic feedforward neural networks (SFNN) have several advantages beyond deterministic deep neural networks (DNN): they have more expressive power allowing multi-modal mappings and regularize better due to their stochastic nature. However, training SFNN is notoriously harder. In this paper, we aim at developing efficient training methods for large-scale SFNN, in particular using known architectures and pre-trained parameters of DNN. To this end, we propose a new intermediate stochastic model, called Simplified-SFNN, which can be built upon any baseline DNN and approximates certain SFNN by simplifying its upper latent units above stochastic ones. The main novelty of our approach is in establishing the connection between three models, i.e., DNN → Simplified-SFNN → SFNN, which naturally leads to an efficient training procedure of the stochastic models utilizing pre-trained parameters of DNN. Using several popular DNNs, we show how they can be effectively transferred to the corresponding stochastic models for both multi-modal and classification tasks on MNIST, TFD, CIFAR-10, CIFAR-100 and SVHN datasets. In particular, our stochastic model built from the wide residual network has 28 layers and 36 million parameters, where the former consistently outperforms the latter for the classification tasks on CIFAR-10 and CIFAR-100 due to its stochastic regularizing effect.

1 INTRODUCTION: Recently, deterministic deep neural networks (DNN) have demonstrated state-of-the-art performance on many supervised tasks, e.g., speech recognition (Hinton et al., 2012a) and object recognition (Krizhevsky et al., 2012). One of the main components underlying these successes is on the efficient training methods for deeper and wider DNNs, which include backpropagation (Rumelhart et al., 1988), stochastic gradient descent (Robbins & Monro, 1951), dropout/dropconnect (Hinton et al., 2012b; Wan et al., 2013), batch/weight normalization (Ioffe & Szegedy, 2015; Salimans & Kingma, 2016), and various activation functions (Nair & Hinton, 2010; Gulcehre et al., 2016). On the other hand, stochastic feedforward neural networks (SFNN) (Neal, 1990) having random latent units are often necessary in order to model complex stochastic natures in many real-world tasks, e.g., structured prediction (Tang & Salakhutdinov, 2013), image generation (Goodfellow et al., 2014) and memory networks (Zaremba & Sutskever, 2015). Furthermore, it has been believed that SFNN has several advantages beyond DNN (Raiko et al., 2014): it has more expressive power for multi-modal learning and regularizes better for large-scale learning.
Training large-scale SFNN is notoriously hard since backpropagation is not directly applicable. Certain stochastic neural networks using continuous random units are known to be trainable efficiently using backpropagation under the variational techniques and the reparameterization tricks (Kingma & Welling, 2013). On the other hand, training SFNN having discrete, i.e., binary or multi-modal, random units is more difficult since intractable probabilistic inference is involved requiring too many random samples. There have been several efforts developing efficient training methods for SFNN having binary random latent units (Neal, 1990; Saul et al., 1996; Tang & Salakhutdinov, 2013; Bengio et al., 2013; Raiko et al., 2014; Gu et al., 2015) (see Section 2.1 for more details). However, training SFNN is still significantly slower than doing DNN of the same architecture, e.g., most prior
works on this line have considered a small number (at most 5 or so) of layers in SFNN. We aim for the same goal, but our direction is orthogonal to them.
Instead of training SFNN directly, we study whether pre-trained parameters of DNN (or easier models) can be transferred to it, possibly with further fine-tuning of light cost. This approach can be attractive since one can utilize recent advances in DNN on its design and training. For example, one can design the network structure of SFNN following known specialized ones of DNN and use their pre-trained parameters. To this end, we first try transferring pre-trained parameters of DNN using sigmoid activation functions to those of the corresponding SFNN directly. In our experiments, the heuristic reasonably works well. For multi-modal learning, SFNN under such a simple transformation outperforms DNN. Even for the MNIST classification, the former performs similarly as the latter (see Section 2 for more details). However, it is questionable whether a similar strategy works in general, particularly for other unbounded activation functions like ReLU (Nair & Hinton, 2010) since SFNN has binary, i.e., bounded, random latent units. Moreover, it lost the regularization benefit of SFNN: it is rather believed that transferring parameters of stochastic models to DNN helps its regularization, but the opposite direction is unlikely possible.
To address the issues, we propose a special form of stochastic neural networks, named SimplifiedSFNN, which intermediates between SFNN and DNN, having the following properties. First, Simplified-SFNN can be built upon any baseline DNN, possibly having unbounded activation functions. The most significant part of our approach lies in providing rigorous network knowledge transferring (Chen et al., 2015) between Simplified-SFNN and DNN. In particular, we prove that parameters of DNN can be transformed to those of the corresponding Simplified-SFNN while preserving the performance, i.e., both represent the same mapping and features. Second, Simplified-SFNN approximates certain SFNN, better than DNN, by simplifying its upper latent units above stochastic ones using two different non-linear activation functions. Simplified-SFNN is much easier to train than SFNN while utilizing its stochastic nature for regularization.
The above connection DNN→ Simplified-SFNN→ SFNN naturally suggests the following training procedure for both SFNN and Simplified-SFNN: train a baseline DNN first and then fine-tune its corresponding Simplified-SFNN initialized by the transformed DNN parameters. The pre-training stage accelerates the training task since DNN is faster to train than Simplified-SFNN. In addition, one can also utilize known DNN training techniques such as dropout and batch normalization for fine-tuning Simplified-SFNN. In our experiments, we train SFNN and Simplified-SFNN under the proposed strategy. They consistently outperform the corresponding DNN for both multi-modal and classification tasks, where the former and the latter are for measuring the model expressive power and the regularization effect, respectively. To the best of our knowledge, we are the first to confirm that SFNN indeed regularizes better than DNN. We also construct the stochastic models following the same network structure of popular DNNs including Lenet-5 (LeCun et al., 1998), NIN (Lin et al., 2014) and WRN (Zagoruyko & Komodakis, 2016). In particular, WRN (wide residual network) of 28 layers and 36 million parameters has shown the state-of-art performances on CIFAR-10 and CIFAR-100 classification datasets, and our stochastic models built upon WRN outperform the deterministic WRN on the datasets.
Organization. In Section 2, we focus on DNNs having sigmoid and ReLU activation functions and study simple transformations of their parameters to those of SFNN. In Section 3, we consider DNNs having general activation functions and describe more advanced transformations via introducing a new model, named Simplified-SFNN.

2 SIMPLE TRANSFORMATION FROM DNN TO SFNN: 

2.1 PRELIMINARIES FOR SFNN: Stochastic feedforward neural network (SFNN) is a hybrid model, which has both stochastic binary and deterministic hidden units. We first introduce SFNN with one stochastic hidden layer (and without deterministic hidden layers) for simplicity. Throughout this paper, we commonly denote the bias for unit i and the weight matrix of