Write a review for the following paper.

Abstract: Extending the success of deep neural networks to natural language understanding and symbolic reasoning requires complex operations and external memory. Recent neural program induction approaches have attempted to address this problem, but are typically limited to differentiable memory, and consequently cannot scale beyond small synthetic tasks. In this work, we propose the Manager-ProgrammerComputer framework, which integrates neural networks with non-differentiable memory to support abstract, scalable and precise operations through a friendly neural computer interface. Specifically, we introduce a Neural Symbolic Machine, which contains a sequence-to-sequence neural "programmer", and a nondifferentiable "computer" that is a Lisp interpreter with code assist. To successfully apply REINFORCE for training, we augment it with approximate gold programs found by an iterative maximum likelihood training process. NSM is able to learn a semantic parser from weak supervision over a large knowledge base. It achieves new state-of-the-art performance on WEBQUESTIONSSP, a challenging semantic parsing dataset. Compared to previous approaches, NSM is end-to-end, therefore does not rely on feature engineering or domain specific knowledge.

1 Introduction: Deep neural networks have achieved impressive performance in classification and structured prediction tasks with full supervision such as speech recognition [9] and machine translation [14, 2, 16]. Extending the success to natural language understanding and symbolic reasoning requires the ability to perform complex operations and make use of an external memory. There were several recent attempts to address this problem in neural program induction [7, 12, 13, 11, 19, 8, 1], which learn programs by using a neural sequence model to control a computation component. However, the memories in these models are either low-level (such as in Neural Turing machines[19]), or differentiable so that they can be trained by backpropagation. This makes it difficult to utilize efficient discrete memory in a traditional computer, and limits their application to small synthetic tasks. ∗The long version with more details can be found at https://arxiv.org/abs/1611.00020. †Work done while the author was interning at Google ‡Work done while the author was a visiting scholar at Google
1st Workshop on Neural Abstract Machines & Program Induction (NAMPI), @NIPS 2016, Barcelona, Spain.
ar X
iv :1
61 2.
01 19
7v 1
[ cs
.C L
To better utilize efficient memory and operations, we propose a Manager-Programmer-Computer (MPC) framework for neural program induction, which integrates three components:
1. A "manager" that provides weak supervision through input and a reward signal indicating how well a task is performed. Unlike full supervision, this weak supervision is much easier to obtain at large scale (see an example task in Section 3).
2. A "programmer" that takes natural language as input and generates a program that is a sequence of tokens. The programmer learns from the reward signal and must overcome the hard search problem of finding good programs. (Section 2.2).
3. A "computer" that executes the program. It can use all the operations that can be implemented as a function in a high level programming language like Lisp. The non-differentiable memory enables abstract, scalable and precise operations, but it requires reinforcement learning. It also provides a friendly neural computer interface to help the "programmer" reduce the search space by detecting and eliminating invalid choices (Section 2.1).
Within the MPC framework, we introduce the Neural Symbolic Machine (NSM) and apply it to semantic parsing. NSM contains a sequence-to-sequence neural network model ("programmer") augmented with a key-variable memory to save and reuse intermediate results for compositionality, and a non-differentiable Lisp interpreter ("computer") that executes programs against a large knowledge base. As code assist, the "computer" also helps reduce the search space by checking for syntax and semantic errors. Compared to existing neural program induction approaches, the efficient memory and friendly interface of the "computer" greatly reduce the burden of the "programmer" and enable the model to perform competitively on real applications. On the challenging semantic parsing dataset WEBQUESTIONSSP [18], NSM achieves new state-of-the-art results with weak supervision. Compared to previous work, it is end-to-end, therefore does not require any feature engineering or domain-specific knowledge.

2 Neural Symbolic Machines: Now we describe in details a Neural Symbolic Machine that falls into the MPC framework, and how it is applied to learn semantic parsing from weak supervision.
Semantic parsing is defined as follows: given a knowledge base (KB) K, and a question q = (w1, w2, ..., wk), produce a program or logical form z that when executed against K generates the right answer y. Let E denote a set of entities (e.g., ABELINCOLN)4, and let P denote a set of properties (or relations, e.g., PLACEOFBIRTHOF). A knowledge base K is a set of assertions (e1, p, e2) ∈ E × P × E , such as (HODGENVILLE, PLACEOFBIRTHOF, ABELINCOLN)).

2.1 "Computer": Lisp interpreter with code assist: Operations learned by current neural network models with differentiable memory, such as addition or sorting, do not generalize perfectly to inputs that are larger than previously observed ones [7, 13]. In contrast, operations implemented in ordinary programming language are abstract, scalable, and precise, because no matter how large the input is or whether it has been seen or not, they will be processed precisely. Based on this observation, we implement all the operations necessary for semantic parsing with ordinary non-differentiable memory, and allow the "programmer" to use them with a high level general purpose programming language.
We adopt a Lisp interpreter with predefined functions listed in 1 as the "computer". The programs that can be executed by it are equivalent to the limited subset of λ-calculus in [17], but easier for a sequence-to-sequence model to generate given Lisp’s simple syntax. Because Lisp is a generalpurpose and high level language, it is easy to extend the model with more operations, which can be implemented as new functions, and complex constructs like control flows and loops.
A program C is a list of expressions (c1...cN ). Each expression is either a special token "RETURN" indicating the end of the program, or a list of tokens enclosed by parentheses "( F A0 ... AK )". F is one of the functions in Table 1, which take as input a list of arguments of specific types, and, when executed, returns the denotation of this expression in K, which is typically a list of entities, and saves
4We also consider numbers (e.g., “1.33”) and date-times (e.g., “1999-1-1”) as entities.
it in a new variable. Ak is F ’s kth argument, which can be either a relation p ∈ P or a variable v. The variables hold the results from previous computations, which can be either a list of entities from executing an expression or an entity resolved from the natural language input.
To create a better neural computer interface, the interpreter provides code assist by producing a list of valid tokens for the "programmer" to pick from at each step. First, a valid token should not cause a syntax error, which is usually checked by modern compilers. For example, if the previous token is "(", the next token must be a function, and if the previous token is "Hop", the next token must be a variable. More importantly, a valid token should not cause a semantic error or run-time error, which can be detected by the interpreter using the value or denotation of previous expressions. For example, given that the previously generated tokens are "(", "Hop", "v", the next available token is restricted to the set of relations that are reachable from entities in v. By providing this neural computer interface, the interpreter reduces the "programmer"’s search space by orders of magnitude, and enables weakly supervised learning on a large knowledge base.

2.2 "Programmer": key-variable memory augmented Seq2Seq model: The "computer" implements the operations (functions) and stores the values (intermediate results) in variables, which simplifies the task for the "programmer". The "programmer" only needs to map natural language into a program, which is a sequence of tokens that references operations and values in the "computer". We use a standard sequence-to-sequence model with attention and augment it with a key-variable memory to reference the values.
A typical sequence-to-sequence model consists of two RNNs