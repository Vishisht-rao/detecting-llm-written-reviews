Write a review for the following paper.

Abstract: We exhibit a strong link between frequentist PAC-Bayesian bounds and the Bayesian marginal likelihood. That is, for the negative log-likelihood loss function, we show that the minimization of PAC-Bayesian generalization bounds maximizes the Bayesian marginal likelihood. This provides an alternative explanation to the Bayesian Occam’s razor criteria, under the assumption that the data is generated by a i.i.d. distribution. Moreover, as the negative log-likelihood is an unbounded loss function, we motivate and propose a PAC-Bayesian theorem tailored for the sub-Gamma loss family, and we show that our approach is sound on classical Bayesian linear regression tasks.

1 Introduction: Since its early beginning [Shawe-Taylor and Williamson, 1997], the PAC-Bayesian theory claims to provide “PAC guarantees to Bayesian algorithms” [McAllester, 1999]. However, despite the amount of work dedicated to this statistical learning theory—many authors improved the initial results1 and/or generalized them for various machine learning setups2—it is mostly used as a frequentist method. That is, under the assumptions that the learning samples are i.i.d.-generated by a data-distribution, this theory expresses probably approximately correct (PAC) bounds on the generalization risk. In other words, with probability 1−δ, the generalization risk is at most ε away from the training risk. The Bayesian side of PAC-Bayes comes mostly from the fact that these bounds are expressed on the averaging/aggregation/ensemble of multiple predictors (weighted by a posterior distribution) and incorporate prior knowledge. Although it is still sometimes referred as a theory that bridges the Bayesian and frequentist approach [e.g., Guyon et al., 2010], it has been merely used to explicitly justify Bayesian methods until now.3
In this work, we provide (up to our knowledge) the first direct connection between Bayesian inference techniques [summarized by Ghahramani, 2015] and PAC-Bayesian theory in a general setup. Our study is based on a simple but insightful connection between the Bayesian marginal likelihood and PAC-Bayesian bounds, that we obtain by considering the negative log-likelihood loss function (Section 3). By doing so, we provide an alternative explanation for the Bayesian Occam’s razor criteria [Jeffreys and Berger, 1992, MacKay, 1992] in the context of model selection, explained as the complexity-accuracy trade-off appearing in most PAC-Bayesian results. In Section 4, we extend PAC-Bayes theorems to regression problems with unbounded loss, adapted to the negative log likelihood loss function. Finally, we study the Bayesian model selection from a PAC-Bayesian perspective (Section 5), and illustrate our finding on classical Bayesian regression tasks (Section 6).
1Seeger [2003], McAllester [2003], Catoni [2007], Lever et al. [2013], Tolstikhin and Seldin [2013], etc. 2Langford and Shawe-Taylor [2002], Seldin and Tishby [2010], Seldin et al. [2011, 2012], Bégin et al. [2014], Pentina and Lampert [2014], etc. 3Some indirect connections can be found in Seeger [2002], Banerjee [2006], Zhang [2006], Lacoste [2015], Bissiri et al. [2016]. See also Ng and Jordan [2001], Meir and Zhang [2003], Grünwald and Langford [2007], Lacoste-Julien et al. [2011] for other studies drawing links between frequentist statistics and Bayesian inference.
ar X
iv :1
60 5.
08 63
6v 1
[ st
at .M
L ]
2 7
M ay
2 01

2 PAC-Bayesian Theory: We denote the learning sample (X,Y )={(xi, yi)}ni=1∈(X×Y)n, that contains n input-output pairs. The main assumption of frequentist learning theories—including PAC-Bayes—is that (X,Y ) is randomly sampled from a data generating distribution that we denoteD. Thus, we denote (X,Y )∼Dn the i.i.d. observation of n elements. From a frequentist perspective, we consider in this work loss functions ` : F×X×Y → R, where F is a (discrete or continuous) set of predictors f : X → Y , and we write the empirical risk on the sample (X,Y ) and the generalization error on distribution D as
L̂ `X,Y (f) = 1
n n∑ i=1 `(f, xi, yi) ; L `D(f) = E (x,y)∼D `(f, x, y) .
The PAC-Bayesian theory [McAllester, 1999, 2003] studies an averaging of the above losses according to a posterior distribution ρ̂ overF . That is, it provides probably approximately correct generalization bounds on the (unknown) quantity Ef∼ρ̂ L `D(f) = Ef∼ρ̂ E(x,y)∼D `(f, x, y) , given the empirical estimate Ef∼ρ̂ L̂ `X,Y (f) and some other parameters. Among these, most PAC-Bayesian theorems rely on the Kullback-Leibler divergence KL(ρ̂‖π) = Ef∼ρ̂ ln[ρ̂(f)/π(f)] between a prior distribution π over F—specified before seeing the learning sample X,Y—and the posterior ρ̂—typically obtained by feeding a learning process with X,Y .
Two appealing aspects of PAC-Bayesian theorems are that they provide data-driven generalization bounds that are computed on the training sample (i.e., they do not rely on a testing sample) and that are uniformly valid for all ρ̂ over F . This explains why many works study them as model selection criteria or as an inspiration for learning algorithm conception. Theorem 1, due to Catoni [2007], has been used to derive or study learning algorithms in Germain et al. [2009], McAllester and Keshet [2011], Hazan et al. [2013], Noy and Crammer [2014]. Theorem 1 (Catoni, 2007). Given a distribution D over X × Y , a hypothesis set F , a loss function `′ : F × X × Y → [0, 1], a prior distribution π over F , a δ ∈ (0, 1], and a real number β > 0, with probability at least 1− δ over the choice of (X,Y ) ∼ Dn, we have
∀ρ̂ on F : E f∼ρ̂ L ` ′ D (f) ≤ 1 1− e−β
[ 1− e−β Ef∼ρ̂ L̂ `′ X,Y (f)− 1 n ( KL(ρ̂‖π)+ ln 1δ )] . (1)
Theorem 1 is limited to loss functions mapping to the range [0, 1]. Through a straightforward rescaling we can extend it to any bounded loss, i.e., ` : F ×X ×Y → [a, b], where [a, b] ⊂ R. This is done by using β := b− a and with the rescaled loss function `′(f, x, y) := (`(f, x, y)−a)/(b−a) ∈ [0, 1] . After few arithmetic manipulations, we can rewrite Equation (1) as
∀ρ̂ on F : E f∼ρ̂ L `D(f) ≤ a+ b−a1−ea−b
[ 1− exp ( −E f∼ρ̂ L̂ `X,Y (f)+a− 1n ( KL(ρ̂‖π)+ ln 1δ ))] . (2)
From an algorithm design perspective, Equation (2) suggests optimizing a trade-off between the empirical expected loss and the Kullback-Leibler divergence. Indeed, for fixed π, X , Y , n, and δ