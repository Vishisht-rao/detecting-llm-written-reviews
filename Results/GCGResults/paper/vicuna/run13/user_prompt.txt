Write a review for the following paper.

Abstract: To cope with the high level of ambiguity faced in domains such as Computer<lb>Vision or Natural Language processing, robust prediction methods often search<lb>for a diverse set of high-quality candidate solutions or proposals. In structured<lb>prediction problems, this becomes a daunting task, as the solution space (image<lb>labelings, sentence parses, etc.) is exponentially large. We study greedy algo-<lb>rithms for finding a diverse subset of solutions in structured-output spaces by<lb>drawing new connections between submodular functions over combinatorial item<lb>sets and High-Order Potentials (HOPs) studied for graphical models. Specifically,<lb>we show via examples that when marginal gains of submodular diversity functions<lb>allow structured representations, this enables efficient (sub-linear time) approxi-<lb>mate maximization by reducing the greedy augmentation step to inference in a<lb>factor graph with appropriately constructed HOPs. We discuss benefits, trade-<lb>offs, and show that our constructions lead to significantly better proposals.

1 Introduction: Many problems in Computer Vision, Natural Language Processing and Computational Biology involve mappings from an input space X to an exponentially large space Y of structured outputs. For instance, Y may be the space of all segmentations of an image with n pixels, each of which may take L labels, so |Y| = Ln. Formulations such as Conditional Random Fields (CRFs) [25], Max-Margin Markov Networks (M3N) [33], and Structured Support Vector Machines (SSVMs) [34] have successfully provided principled ways of scoring all solutions y ∈ Y and predicting the single highest scoring or maximum a posteriori (MAP) configuration, by exploiting the factorization of a structured output into its constituent “parts”.
In a number of scenarios, the posterior P(y|x) has several modes due to ambiguities, and we seek not only a single best prediction but a set of good predictions: (1) Interactive Machine Learning. Systems like Google Translate (for machine translation) or Photoshop (for interactive image segmentation) solve structured prediction problems that are often ambiguous ("what did the user really mean?"). Generating a small set of relevant candidate solutions for the user to select from can greatly improve the results. (2) M-Best hypotheses in cascades. Machine learning algorithms are often cascaded, with the output of one model being fed into another [35]. Hence, at the initial stages it is not necessary to make a single perfect prediction. We rather seek a set of plausible predictions that are subsequently re-ranked, combined or processed by a more sophisticated mechanism. In both scenarios, we ideally want a small set of M plausible (i.e., high scoring) but non-redundant (i.e., diverse) structured-outputs to hedge our bets.
Submodular Maximization and Diversity. The task of searching for a diverse high-quality subset of items from a ground set V has been well-studied in information retrieval [5], sensor placement [23], document summarization [27], viral marketing [18], and robotics [10]. Across these domains, submodularity has emerged as an a fundamental and practical concept – a property of functions for measuring diversity of a subset of items. Specifically, a set function F : 2V → R is submodular if its marginal gains, F (a|S) ≡ F (S∪a)−F (S) are decreasing, i.e. F (a|S) ≥ F (a|T )
ar X
iv :1
41 1.
17 52
v1 [
cs .L
G ]
6 N
ov 2
01 4
for all S ⊆ T and a /∈ T . In addition, if F is monotone, i.e., F (S) ≤ F (T ), ∀S ⊆ T , then a simple greedy algorithm (that in each iteration t adds to the current set St the item with the largest marginal gain F (a|St)) achieves an approximation factor of (1 − 1e ) [28]. This result has had significant practical impact [22]. Unfortunately, if the number of items |V | is exponentially large, then even a single linear scan for greedy augmentation is infeasible.
In this work, we study conditions under which it is feasible to greedily maximize a submodular function over an exponentially large ground set V = {v1, . . . , vN} whose elements are combinatorial objects, i.e., labelings of a base set of n variables y = {y1, y2, . . . , yn}. For instance, in image segmentation, the base variables yi are pixel labels, and each item a ∈ V is a particular labeling of the pixels. Or, if each base variable ye indicates the presence or absence of an edge e in a graph, then each item may represent a spanning tree or a maximal matching. Our goal is to find a set of M plausible and diverse configurations efficiently, i.e. in time sub-linear in |V | (ideally scaling as a low-order polynomial in log |V |). We will assume F (·) to be monotone submodular, nonnegative and normalized (F (∅) = 0), and base our study on the greedy algorithm. As a running example, we focus on pixel labeling, where each base variable takes values in a set [L] = {1, . . . , L} of labels. Contributions. Our principal contribution is a conceptual one. We observe that marginal gains of a number of submodular functions allow structured representations, and this enables efficient greedy maximization over exponentially large ground sets – by reducing the greedy augmentation step to a MAP inference query in a discrete factor graph augmented with a suitably constructed HighOrder Potential (HOP). Thus, our work draws new connections between two seemingly disparate but highly related areas in machine learning – submodular maximization and inference in graphical models with structured HOPs. As specific examples, we construct submodular functions for three different, task-dependent definitions of diversity, and provide reductions to three different HOPs for which efficient inference techniques have already been developed. Moreover, we present a generic recipe for constructing such submodular functions, which may be “plugged” with efficient HOPs discovered in future work. Our empirical contribution is an efficient algorithm for producing a set of image segmentations with significantly higher oracle accuracy1 than previous works. The algorithm is general enough to transfer to other applications. Fig. 1 shows an overview of our approach.
Related work: generating multiple solutions. Determinental Point Processesare an elegant probabilistic model over sets of items with a preference for diversity. Its generalization to a structured setting [24] assumes a tree-structured model, an assumption that we do not make. Guzman-Rivera et al. [15, 16] learn a set of M models, each producing one solution, to form the set of solutions. Their approach requires access to the learning sub-routine and repeated re-training of the models, which is not always possible, as it may be expensive or proprietary. We assume to be given a single (pretrained) model from which we must generate multiple diverse, good solutions. Perhaps the closest to our setting are recent techniques for finding diverse M -best solutions [2, 29] or modes [7, 8] in graphical models. While [7] and [8] are inapplicable since they are restricted to chain and tree graphs, we compare to other baselines in Section 3.2 and 4.

1.1 Preliminaries and Notation: We select from a ground set V of N items. Each item is a labeling y = {y1, y2, . . . , yn} of n base variables. For clarity, we use non-bold letters a ∈ V for items, and boldface letters y for base set configurations. Uppercase letters refer to functions over the ground set items F (a|A), R(a|A), D(a|A), and lowercase letters to functions over base variables f(y), r(y), d(y).
1The accuracy of the most accurate segmentation in the set.
Formally, there is a bijection φ : V 7→ [L]m that maps items a ∈ V to their representation as base variable labelings y = φ(a). For notational simplicity, we often use y ∈ S to mean φ−1(y) ∈ S, i.e. the item corresponding to the labeling y is present in the set S