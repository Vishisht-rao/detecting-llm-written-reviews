Write a review for the following paper.

Abstract: Bayesian model averaging (BMA) is a common approach to average over alternative models; yet, it usually gets excessively concentrated around the single most probable model, therefore achieving only sub-optimal classification performance. The compression-based approach (Boullé, 2007) overcomes this problem; it averages over the different models by applying a logarithmic smoothing over the models’ posterior probabilities. This approach has shown excellent performances when applied to ensembles of naive Bayes classifiers. AODE is another ensemble of models with high performance (Webb et al., 2005): it consists of a collection of non-naive classifiers (called SPODE) whose probabilistic predictions are aggregated by simple arithmetic mean. Aggregating the SPODEs via BMA rather than by arithmetic mean deteriorates the performance; instead, we propose to aggregate the SPODEs via the compression coefficients and we show that the resulting classifier obtains a slight but consistent improvement over AODE. However, an important issue in any Bayesian ensemble of models is the arbitrariness in the choice of the prior over the models. We address this problem by adopting the paradigm of credal classification, namely by substituting the unique prior with a set of priors. Credal classifier are able to automatically recognize the prior-dependent instances, namely the instances whose most probable class varies, when different priors are considered; in these cases, credal classifiers remain reliable by returning a set of classes rather than a single class. We thus develop the credal version Corresponding author: giorgio@idsia.ch Preprint submitted to Elsevier January 1, 2014 of both the BMA-based and the compression-based ensemble of SPODEs, substituting the single prior over the models by a set of priors. By experiments we show that both credal classifiers provide overall higher classification reliability than their determinate counterparts. Moreover, the compression-based credal classifier compares favorably to previous credal classifiers.

1. Introduction: Bayesian model averaging (BMA) (Hoeting et al., 1999) is a sound solution to the uncertainty which characterizes the identification of the supposedly best model for a certain data set; given a set of alternative models, BMA weights the inferences produced by the various models, using the models’ posterior probabilities as weights. BMA assumes the data to be generated by one of the considered models; under this assumption, it provides better predictive accuracy than any single model (Hoeting et al., 1999). However, such an assumption is generally not true; for this reason, on real data sets BMA does not generally perform very well; see the discussion and the references in Cerquides et al. (2005) for more details. The problem is that BMA gets excessively concentrated around the single most probable model (Domingos, 2000; Minka, 2002): especially on large data sets, “averaging using the posterior probabilities to weight the models is almost the same as selecting the MAP model” (Boullé, 2007). To overcome the problem of BMA getting excessively concentrated around the most probable model, a compression-based approach has been introduced in (Boullé, 2007); it computes more evenly-distributed weights, by applying a logarithmic smoothing to the models posterior probabilities. The compression-based weights, which can be justified from an information-theoretic viewpoint, have been used in Boullé (2007) to average over different naive Bayes classifiers, characterized by different feature sets, obtaining excellent rank in international competitions on classification.
Another ensemble of Bayesian networks classifiers known for its good performance is AODE (Webb et al., 2005), which is instead based on a set of SPODE (SuperParent-
One-Dependence Estimator) models. Each SPODE adopts a certain feature as a superparent, namely it models all the remaining features as depending on both the class and the super-parent. AODE then simply averages the posterior probabilities computed by the different SPODEs. Alternative methods to aggregate SPODEs, more complex than AODE, have been considered (Yang et al., 2007), but AODE generally outperforms them: “AODE, which simply linearly combines every SPODE without any selection or weighting, is actually more effective than the majority of rival schemes”. As reported in (Cerquides et al., 2005; Yang et al., 2007), AODE outperforms aggregating SPODEs via BMA; in both (Yang et al., 2007; Cerquides et al., 2005) the best results were instead obtained using an algorithm (called MAPLMG), which estimates the most probable linear mixture of SPODEs; this overcomes the problem of assuming a single SPODE to be the true model. In this paper, we address this problem by means of the compression coefficients.
As a preliminary step we develop BMA-AODE, namely BMA over SPODEs, with some computational differences with respect to the framework of Yang et al. (2007) and Cerquides et al. (2005); our results confirm however that BMA over SPODEs is outperformed by AODE. Then we develop the novel COMP-AODE classifier, which weights the SPODEs using the compression-based coefficients, and we show that it yields a slight but consistent improvement in the classification performance over the standard AODE. Considering the high performance of AODE, we regard this result as noteworthy.
An important issue in any Bayesian ensemble is choosing the prior over the models. A common choice is to adopt a uniform mass function, as we do in both BMA-AODE and COMP-AODE; this however can be criticized from different standpoints; see for instance the rejoinder in Hoeting et al. (1999). In Boullé (2007), a prior which favors simpler models over complex ones is adopted. Although all these choices are reasonable, the specification of any single prior implies some arbitrariness, which entails the risk of prior-dependent, and hence potentially fragile, conclusions.
In fact, the specification of the prior over the models is a serious open problem for Bayesian ensembles of models. We address this problem by adopting the paradigm of credal classification (Corani et al., 2012; Corani and Zaffalon, 2008b), namely drop-
ping the unique prior in favor of a set of priors (prior credal set) (Levi, 1980). While a traditional non-informative priors represents a condition of indifference between the alternative models, a credal set describes a condition of prior ignorance, letting thus vary the prior probability of each model over a wide interval, instead of fixing it to a specific number. Credal classifiers are able to automatically detect the instances whose most probable class varies when different priors are considered; such instances are called prior-dependent. Credal classifiers remain reliable on prior-dependent instances by returning a set of classes; traditional classifiers have instead typically low accuracy on the prior-dependent instances (Corani and Zaffalon, 2008a,b).
We then develop BMA-AODE* and COMP-AODE*, namely the credal counterparts of respectively BMA-AODE and COMP-AODE. By extensive experiments we show that both credal classifiers are sensible extension of their single-prior counterparts; in fact, they return a small-sized but highly accurate set of classes on the priordependent instances, over which instead their single-prior counterparts have reduced accuracy. We conclude by showing that COMP-AODE* compares favorably to both BMA-AODE* and other existing credal classifiers.

2. Methods: We consider a classification problem with k features; we denote by C the class variable (taking values in C) and by A := (A1, . . . , Ak) the set of features, taking values respectively in A1, . . . ,Ak. For a generic variable A, we denote as P (A) the probability mass function over its values and as P (a) the probability that A = a. We assume the data to be complete and the training data D to contain n instances. We learn the model parameters from the training data by adopting Dirichlet priors and setting the equivalent sample size to 1. Under 0-1 loss a traditional probabilistic classifier returns, for a test instance ã = {ã1, . . . , ãk} whose class is unknown, the most