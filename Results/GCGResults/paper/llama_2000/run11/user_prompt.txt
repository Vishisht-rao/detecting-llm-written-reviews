Write a review for the following paper.

Abstract: We study two mixed robust/average-case submodular partitioning problems that we collectively call Submodular Partitioning. These problems generalize both purely robust instances of the problem (namely max-min submodular fair allocation (SFA) Golovin (2005) and min-max submodular load balancing (SLB) Svitkina and Fleischer (2008)) and also generalize average-case instances (that is the submodular welfare problem (SWP) Vondrák (2008) and submodular multiway partition (SMP) Chekuri and Ene (2011a)). While the robust versions have been studied in the theory community Goemans et al. (2009); Golovin (2005); Khot and Ponnuswami (2007); Svitkina and Fleischer (2008); Vondrák (2008), existing work has focused on tight approximation guarantees, and the resultant algorithms are not, in general, scalable to very large real-world applications. This is in contrast to the average case, where most of the algorithms are scalable. In the present paper, we bridge this gap, by proposing several new algorithms (including those based on greedy, majorization-minimization, minorization-maximization, and relaxation algorithms) that not only scale to large sizes but that also achieve theoretical approximation guarantees close to the state-of-the-art, and in some cases achieve new tight bounds. We also provide new scalable algorithms that apply to additive combinations of the robust and average-case extreme objectives. We show that these problems have many applications in machine learning (ML). This includes: 1) data 1 ar X iv :1 51 0. 08 86 5v 2 [ cs .D S] 1 6 A ug 2 01 6 Wei, Iyer, Wang, Bai, Bilmes partitioning and load balancing for distributed machine algorithms on parallel machines; 2) data clustering; and 3) multi-label image segmentation with (only) Boolean submodular functions via pixel partitioning. We empirically demonstrate the efficacy of our algorithms on real-world problems involving data partitioning for distributed optimization of standard machine learning objectives (including both convex and deep neural network objectives), and also on purely unsupervised (i.e., no supervised or semi-supervised learning, and no interactive segmentation) image segmentation.

1. Introduction: The problem of set partitioning arises in many machine learning (ML) and data science applications. Given a set V of items, an m-partition π = (Aπ1 , A π 2 , . . . , A π m) is a size m set of subsets of V (called blocks) that are non-intersecting (i.e., Aπi ∩ Aπj = ∅ for all i 6= j) and covering (i.e., ⋃m i=1A π i = V ). The goal of a partitioning algorithm is to produce a partitioning that is measurably good in some way, often based on an aggregation of the judgements of the internal goodness of the resulting blocks.
In data science and machine learning applications, a partitioning is almost always the end result of a clustering (although in some cases a clustering might allow for intersecting subsets) in which V is partitioned into m clusters (which we, in this paper, refer to as blocks). Most clustering problems are based on optimizing an objective that is either a sum, or an average-case, utility where the goal is to optimize the sum of individual cluster costs — this includes the ubiquitous k-means procedure Lloyd (1982); Arthur and Vassilvitskii (2007) where the goal is to construct a partitioning based on choosing a set of cluster centers that minimizes the total sum of squared distances between each point and its closest center. More rarely, clustering algorithms may be based on robust objective functions Garćıa-Escudero et al. (2010), where the goal is to optimize the worst-case internal cluster cost (i.e., quality of the clustering is based solely on the quality of the worst internal cluster cost within the clustering). The average case vs. worst case cluster cost assessment distinction are extremes along a continuum, although all existing algorithms operate only at, rather than in between, these extremes.
There is another way of categorizing clustering algorithms, and that based on the goal of each resultant cluster. Most of the time, clustering algorithms attempt to produce clusters containing items similar to or near each other (e.g., with k-means, a cluster consists of a centroid and a set of nearby points), and dissimilarity exists, ideally, only between different clusters. An alternate possible goal of clustering is to have each block contains as diverse a set of items as possible, where similarity exist between rather than within clusters. For example, partitioning the vertices of a graph into a set of k independent (or, equivalently, stable) sets would fall into this later category, assuming the graph has edges only between similar vertices. This subsumes graph k-colorability problems, one of the most well-known of the NP-complete problems, although in some special cases it is solvable in polynomial time Brandstädt (1996); Feder et al. (1999); Hell et al. (2004). This general idea, where the clusters are as internally diverse as possible using some given measure of diversity, has been called anticlustering Valev (1983, 1998); Späth (1986) in the past and, as can be seen from the above, comprises in general some difficult computational problems.
A third way of further delineating cluster problems is based on how each cluster’s internal quality is measured. In most clustering procedures, each cluster Aπi for i ∈ {1, 2, . . . ,m} is internally judged based on the same function f regardless of which cluster is being evaluated. The overall clustering is then an aggregation of the vector of m scores f(Aπ1 ), f(A π 2 ), . . . , f(A π m). We call this strategy a homogeneous clustering evaluation since every cluster is internally evaluated using the same underlying function. An alternate strategy to evaluate a clustering uses a different function for each cluster. For example, in assignment problems, there are m individuals each of whom is assigned a cluster, and each individual might not judge the same cluster identically. In such cases, we have m functions f1, f2, . . . , fm that, respectively, internally evaluate the corresponding cluster, and the overall clustering evaluation is based on an aggregation of the vector of scores f1(A π 1 ), f2(A π 2 ), . . . , fm(A π m). We call this a heterogeneous clustering evaluation.
The above three strategies to determine the form of cluster evaluation (i.e., average vs. robust case, internally similar vs. diverse clusters, and homogeneous vs. heterogeneous internal cluster evaluation) combined with the various ways of judging the internal cluster evaluations, and ways to aggregate the scores, leads to a plethora of possible clustering objectives. Even in the simple cases, however (such as k-means, or independent sets of graphs), the problems are generally hard and/or difficult to approximate.
This paper studies partitioning problems that span the range within and across the aforementioned three strategies, and all from the perspective of submodular function based internal cluster evaluations. In particular, we study problems of the following form:
Problem 1: max π∈Π
[ (1− λ) min
i fi(A
π i ) +
λ
m m∑ j=1 fj(A π j ) ] , (1)
and
Problem 2: min π∈Π
[ (1− λ) max
i fi(A
π i ) +
λ
m m∑ j=1 fj(A π j ) ] , (2)
where 0 ≤ λ ≤ 1, the set of sets π = (Aπ1 , Aπ2 , · · · , Aπm) is an ordered partition of a finite set V (i.e, ∪iAπi = V and ∀i 6= j, Aπi ∩Aπj = ∅), and Π refers to the set of all ordered partitions of V into m blocks. In contrast to the notion of the partition often used in the computer science and mathematical communities, we clarify that an ordered partition π is fully characterized by both its constituent blocks {Aπi }mi=1 as well as the ordering of the blocks — this allows us to cover