Write a review for the following paper.

Abstract: We present doubly stochastic gradient MCMC, a simple and generic method for (approximate) Bayesian inference of deep generative models (DGMs) in a collapsed continuous parameter space. At each MCMC sampling step, the algorithm randomly draws a mini-batch of data samples to estimate the gradient of log-posterior and further estimates the intractable expectation over hidden variables via a neural adaptive importance sampler, where the proposal distribution is parameterized by a deep neural network and learnt jointly. We demonstrate the effectiveness on learning various DGMs in a wide range of tasks, including density estimation, data generation and missing data imputation. Our method outperforms many state-of-the-art competitors.

1. Introduction: Learning deep models that consist of multi-layered representations has obtained state-of-the-art performance in many tasks (Bengio et al., 2014; Hinton et al., 2006), partly due to their ability on capturing high-level abstractions. As an important family of deep models, deep generative models (DGMs) (Hinton et al., 2006; Salakhutdinov & Hinton, 2009) can answer a wide range of queries by performing probabilistic inference, such as inferring the missing values of input data, which is beyond the scope of recognition networks such as deep neural networks.
However, probabilistic inference with DGMs is challenging, especially when a Bayesian formalism is adopted, which is desirable to protect the DGM from overfitting (MacKay, 1992; Neal, 1995) and to perform sparse Bayesian inference (Gan et al., 2015b) or nonparametric inference (Adams et al., 2010) to learn the network structure. For Bayesian methods in general, the posterior inference often involves intractable integrals because of several potential factors, such as that the space is extremely high-
dimensional and that the Bayesian model is non-conjugate. To address the challenges, approximate methods have to be adopted, including variational (Jordan et al., 1999; Saul et al., 1996) and Markov chain Monte Carlo (MCMC) methods (Robert & Casella, 2005).
Much progress has been made on stochastic variational methods for DGMs (Kingma & Welling, 2014; Rezende et al., 2014; Ranganath et al., 2014), under some mean-field or parameterization assumptions. One key feature of such variational methods is that they marry ideas from deep neural networks to parameterize the variational distribution by a recognition network and jointly learn the parameters by optimizing a variational bound. In contrast, little work has been done on extending MCMC methods to learn DGMs in a Bayesian setting, which are often more accurate, except a few exceptions. Gan et al. (2015b) present a Gibbs sampler for deep sigmoid belief networks with a sparsity-inducing prior via data augmentation, Adams et al. (2010) present a Metropolis-Hastings method for cascading Indian buffet process and Li et al. (2016) develop a high-order stochastic gradient MCMC method and apply to deep Poisson factor analysis (Gan et al., 2015a).
In this paper, we present a simple and generic method, named doubly stochastic gradient MCMC, to improve the efficiency of performing Bayesian inference on DGMs. By drawing samples in the collapsed parameter space, our method extends the recent work on stochastic gradient MCMC (Welling & Teh, 2011; Ahn et al., 2012; Chen et al., 2014; Ding et al., 2014) to deal with the challenging task of posterior inference with DGMs. Besides the stochasticity of randomly drawing a mini-batch of samples in stochastic approximation, our algorithm introduces an extra dimension of stochasticity to estimate the intractable gradients by randomly drawing the hidden variables in DGMs. The sampling can be done via a Gibbs sampler, which however has a low mixing rate in high dimensional spaces. To address that, we develop a neural adaptive importance sampler (NAIS), where the adaptive proposal is parameterized by a recognition network and the parameters are optimized ar X
iv :1
50 6.
04 55
7v 4
[ cs
.L G
] 7
M ar
2 01
by descending inclusive KL-divergence. By combining the two types of stochasticity, we construct an asymptotically unbiased estimate of the gradient in the continuous parameter space. Then, a stochastic gradient MCMC method is applied with guarantee to (approximately) converge to the target posterior when the learning rates are set under some proper annealing scheme.
Our method can be widely applied to the DGMs with either discrete or continuous hidden variables. In experiments, we demonstrate the efficacy on learning various DGMs, such as deep sigmoid belief networks (Mnih & Gregor, 2014), for density estimation, data generation and missing value imputation. Our results show that we can outperform many strong competitors for learning DGMs.

2. Related Work: Recently, there has been a lot of interest in developing variational methods for DGMs. One common strategy for dealing with the intractable posterior distribution is to approximate it with a recognition (or inference) network, and a variational lower bound is then optimized (Kingma & Welling, 2014; Mnih & Gregor, 2014). Note in these methods the gradients are also estimated doubly stochastically. Kingma & Welling (2014) and Mnih & Gregor (2014) adopt variance reduction techniques to make these methods practically applicable. Titsias & Lázaro-Gredilla (2014) propose a so-called “doubly stochastic variational inference” method for non-conjugate Bayesian inference. We are inspired by these methods when naming ours.
The reweighted wake-sleep (RWS) (Bornschein & Bengio, 2015) and importance weighted autoencoder (IWAE) (Burda et al., 2015) directly estimate the log-likelihood (as well as its gradient) via importance sampling, where the proposal distribution is characterized by a recognition model. These methods reduce the gap between the variational bound and the log-likelihood, which is shown much tighter than that in Kingma & Welling (2014). Such tighter bound results in an asymptotically unbiased estimator of its gradient. We draw inspiration from these variational methods to build our MCMC samplers.
Our work is closely related to the recent progress on neural adaptive proposals for sequential Monte Carlo (NASMC) (Gu et al., 2015). Different from our work, NASMC deals with dynamical models such as Hidden Markov models and adopts recurrent neural network as the proposal. We use a similar KL-divergence as NASMC to learn the proposal.
Finally, Gan et al. (2015a) adopt a Monte Carlo estimate via Gibbs sampling to the intractable gradients under a stochastic MCMC method particularly for topic models. Besides a general perspective which is applicable to various types of DGM models, we propose a neural adaptive importance
sampler which is more efficient than Gibbs sampling and leads to better estimates.

3. Doubly Stochastic Gradient MCMC for Deep Generative Models: We now present the doubly stochastic gradient MCMC for deep generative models.

3.1. Deep Generative Models: Let X = {xn}Nn=1 be a given dataset with N i.i.d. samples. A deep generative model (DGM) assumes that each sample xn ∈ RD is generated from a vector of hidden variables zn ∈ RH , which itself follows some prior distribution p(z|α). Let p(x|z,β) be the likelihood model. The joint probability of a DGM is as follows:
p(X,Z|θ) = N∏ n=1 p(zn|α)p(xn|zn,β), (1)
where θ := (α,β). Depending on the structure of z, various DGMs have been developed, such as deep belief networks (Hinton et al., 2006), deep sigmoid belief networks (Mnih & Gregor, 2014), and deep Boltzmann machines (Salakhutdinov & Hinton, 20