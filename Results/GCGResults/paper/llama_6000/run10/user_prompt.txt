Write a review for the following paper.

Abstract: In one-class classification (OCC) problems, only the data for the target class<lb>is available, whereas the data for the non-target class may be completely ab-<lb>sent. In this paper, we study one-class nearest neighbour (OCNN) classifiers<lb>and their different variants for the OCC problem. We present a theoreti-<lb>cal analysis to show the equivalence among different variants of OCNN that<lb>may use different neighbours or thresholds to identify unseen examples of<lb>the non-target class. We also present a method based on inter-quartile range<lb>for optimizing parameters used in OCNN in the absence of non-target data<lb>during training. Then, we propose to use two ensemble approaches based on<lb>random sub-space and random projection approaches to create accurate en-<lb>semble that significantly outperforms the baseline OCNN. We tested the pro-<lb>posed methods on various benchmark and real word domain-specific datasets<lb>to show their superior performance. The results give strong evidence that the<lb>random projection ensemble of the proposed OCNN with optimized param-<lb>eters variants perform significantly and consistently better than the single<lb>OCC on all the tested datasets.

1. Introduction: One-Class Classification (OCC) [1] is a special case of general classification problem where the data of the positive (target) class is sufficiently available, whereas the negative class data is absent during the training time. This deficiency of data for the negative class may arise due several reasons, such as the difficulty in obtaining the data, higher cost in data collection or the rarity of their occurrence. Some examples for OCC are machine fault diagnosis, fraud detection, human fall detection, identifying rare disease. In most of these applications, it is easy to collect samples for the positive (target) class or the data describing the normal behaviour of the domain under consideration. However, the collection of negative samples may result in high cost in dollars, put health and safety of a person in danger. In other cases, the data in the negative class occurs rarely. Therefore, even if some samples are collected for the negative class, the training data will be severely skewed and it is difficult to build generalizable classifiers using the traditional supervised classification algorithms.
When the data for the negative class is too few or absent, a normal practice is to generate artificial data for the negative class [2] and convert the classification problem as a binary classification problem. Another possibility is to estimated the parameters of the distribution of negative class based on varying the parameters of the positive class [3]. However, these techniques heavily depend on the choice of parameters for the distribution of the positive class or the unseen negative class. The literature on OCC offers several alternatives to build a classification boundary based on the positive samples only [4]. In this paper, we choose a one-class nearest neighbour (OCNN) approach for detecting unseen samples of the negative class, when they were not present during the training phase. In principle, an OCNN method finds the high and low density regions based on the local neighbourhood of a test sample. Using a decision threshold, an OCNN accepts or rejects a test sample as a member of the target class. In its simplest form, an OCNN finds the 1st nearest neighbour of a test sample in the target class and then finds the 1st nearest neighbour of this neighbour in the target class. If the ratio of these distances is lower than a user-defined threshold, it is accepted as a member of the target class [5]. This method is simple and effective in finding instances of the unseen negative class; however, it is very sensitive to the
noise in the target class. This method may also be not effective when the dimension of the data is very high because of the problems associated with the euclidean distance metric. In an OCNN, the number of nearest neighbours and the value of decision threshold can be optimized but the sensitivity of the classifier w.r.t. noise in the positive class may change. There exists several variants of OCNN in the literature (see Section 2); however, these methods do not make relation between the other OCNN methods and do not clearly explain the reasons as to why a particular variant works better than the other.
A classifier ensembles is an approach to improve the performance of a classifier [6, 7] Classifier ensembles consist of many classifiers; the final result of a classifier ensemble depends on the combined results of individual classifiers of the ensemble. A classifier ensemble generally performs better than individual classifiers provided the member classifiers are accurate and diverse. As supervised K-Nearest Neighbour (KNN) classifiers are very robust to variations of the data set; therefore, ensemble techniques based on training data sampling (such as Bagging [8] and Boosting [9]) have not been successful to create their ensembles [10]. KNN classifiers are found sensitive to input features sampling; therefore, diverse NN classifiers can be created by using different feature space for different KNN classifiers to generate an accurate ensemble [10].
In this paper, we present a holistic view on the general problem of OCNN by discussing their different variants and creating accurate their ensembles. The main contributions of the paper are:
• We theoretically show the equivalence and relationship between various OCNN approaches and discuss the impact of choosing nearest neighbours on the decision threshold.
• We present two different types of ensemble methods – random subspace and random projection for the OCNN to study their performance when the feature space is changed. To the best of our knowledge, the suitability of random projection approach is investigated for the first time for OCNN.
• We present present a cross-validation method and a modified thresholding algorithm that utilize the outliers from the target class to optimize the parameters of the OCNN.
Our results on several benchmark and domain-specific real world data shows superior performance of the OCNN with random projection ensembles and give strong evidence that single OCNN may not be the right choice to detect unseen negative data during testing.
The rest of the paper is structured as follows. In Section 2, we present literature review on various variants of OCNN and their ensembles. Section 3 introduces two variants of OCNN that uses different numbers of nearest neighbours and decision threshold, followed by a theoretical analysis about their relation with other variants of OCNN and a brief discussion on various ensemble approaches that are used with the OCNN classifiers. In Section 4, we introduce a cross-validation method and a modified thresholding algorithm to optimize parameters for the OCNN classifiers using only the data from the target class. Experimental results are shown on various datasets in Section 5. Conclusions and future work are summarized in Section 6.

2. Related Work: The research on one-class classification spans more than a decade and researchers have attempted various models using support vector machines, nearest neighbours, decision trees, neural networks, bayesian networks and classifier ensembles [4]. These models mostly differ in either learning with target examples only; learning with target examples and some amount of poorly sampled outlier examples or artificially generated outliers; learning with labelled target and unlabelled data; methodology used and application domains applied [1]. In this paper we restrict our literature review to one class nearest neighbour approaches and the methods that involve their ensemble.
Tax [5] presents a one-class nearest neighbour method called nearest neighbour description (NN-d). In NN-d, a test object is accepted as a member of the target class when its local density is larger or equal to the local density of its nearest neighbour in the training set. The general idea is to find the distance of a test object to its nearest neighbour in the target class, and find the nearest neighbour of this neighbour, and if the distance is greater than a threshold then it is identified as an outlier. Tax and Duin [11] propose a nearest neighbour method capable of finding data boundaries when the sample size is very low. The boundary thus constructed can be used to detect the targets and outliers. However, this method has the disadvantage that it relies on the individual positions of the objects in the target set. Their method seems to be useful in situations where the data is distributed in sub-
spaces. They test the technique on both real and artificial data and find it to be useful when very small amounts of training data exist (fewer than 5 samples per feature). Haro-Garcıa et al. [12] use NN-d along with other one-class classifiers for identifying plant/pathogen sequences. They find the suitability of above methods owing to the fact that genomic sequences of plant are easy to obtain in comparison to pathogens. Their basic idea is to optimize the threshold between the distances of a test object to its nearest neighbour and distance of this neighbour to its nearest neighbour. They order these distances and find the best threshold based on the percentile of the sorted sequence. Angiulli [13] present a prototype based domain description algorithm (PDD) for one class classification task