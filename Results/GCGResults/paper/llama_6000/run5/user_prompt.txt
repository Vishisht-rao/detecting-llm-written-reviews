Write a review for the following paper.

Abstract: <lb>In applications such as recommendation systems and revenue management, it is important to<lb>predict preferences on items that have not been seen by a user or predict outcomes of comparisons<lb>among those that have never been compared. A popular discrete choice model of multinomial<lb>logit model captures the structure of the hidden preferences with a low-rank matrix. In order to<lb>predict the preferences, we want to learn the underlying model from noisy observations of the<lb>low-rank matrix, collected as revealed preferences in various forms of ordinal data. A natural<lb>approach to learn such a model is to solve a convex relaxation of nuclear norm minimization.<lb>We present the convex relaxation approach in two contexts of interest: collaborative ranking<lb>and bundled choice modeling. In both cases, we show that the convex relaxation is minimax<lb>optimal. We prove an upper bound on the resulting error with finite samples, and provide a<lb>matching information-theoretic lower bound.

1 Introduction: In many applications such as recommendation systems and revenue management, it is important to predict preferences on items that have not been seen by a user or predict outcomes of comparisons among those that have never been compared. Predicting such hidden preferences would be hopeless without further assumptions on the structure of the preference. Motivated by the success of matrix factorization models on collaborative filtering applications, we model hidden preferences with lowrank matrices to collaboratively learn preference matrices from ordinal data. In this paper, we consider the following two concrete scenarios:
• Collaborative ranking. Consider an online market that collects each user’s preference as a ranking over a subset of items that are ‘seen’ by the user. Such data can be obtained by directly asking to compare some items, or by indirectly tracking online activities on which items are viewed, how much time is spent on the page, or how the user rated the items. In order to make personalized recommendations, we want (a) a model that captures how users who preferred similar items are also likely to have similar preferences on unseen items; and (b) to predict which items the user might prefer, by learning such models from ordinal data.
∗Department of Industrial and Enterprise Systems Engineering, University of Illinois at Urbana-Champaign, email: swoh@illinois.edu †Department of Electrical and Computer Engineering, University of Illinois at Urbana-Champaign, e-mail: thekump2@illinois.edu ‡Statistics Department, University of Pennsylvania, e-mail: jxu18@illinois.edu
ar X
iv :1
50 6.
07 94
7v 1
[ cs
.L G
] 2
6 Ju
n 20
• Bundled choice modeling. Discrete choice models describe how a user makes decisions on what to purchase. Typical choice models assume the willingness to buy an item is independent of what else the user bought. In many cases, however, we make ‘bundled’ purchases: we buy particular ingredients together for one recipe or we buy two connecting flights. One choice (the first flight) has a significant impact on the other (the connecting flight). In order to optimize the assortment (which flight schedules to offer) for maximum expected revenue, it is crucial to accurately predict the willingness of the consumers to purchase, based on past history. We consider a case where there are two types of products (e.g. jeans and shirts), and want (a) a model that captures such interacting preferences for pairs of items, one from each category; and (b) to predict the consumer’s choice probabilities on pairs of items, by learning such models from past purchase history.
We use a discrete choice model known as MultiNomial Logit (MNL) model (described in Section 2.1) to represent the preferences. In collaborative ranking context, MNL uses a low-rank matrix to represent the hidden preferences of the users. Each row corresponds to a user’s preference over all the items, and when presented with a subset of items the user provides a ranking over those items, which is a noisy version of the hidden true preference. the low-rank assumption naturally captures the similarities among users and items, by representing each on a low-dimensional space. In bundled choice modeling context, the low-rank matrix now represents how pairs of items are matched. Each row corresponds to an item from the first category and each column corresponds to an item from the second category. An entry in the matrix represents how much the pair is preferred by a randomly chosen user from a pool of users. Notice that in this case we do not model individual preferences, but the preference of the whole population. The purchase history of the population is the record of which pair was chosen among a subsets of items that were presented, which is again a noisy version of the hidden true preference. The low-rank assumption captures the similarities and dis-similarities among the items in the same category and the interactions across categories.
Contribution. A natural approach to learn such a low-rank model, from noisy observations, is to solve a convex relaxation of nuclear norm minimization (described in Section 2.2). We present such an approach for learning the MNL model from ordinal data, in two contexts: collaborative ranking and bundled choice modeling. In both cases, we analyze the sample complexity of the algorithm, and provide an upper bound on the resulting error with finite samples. We prove minimax-optimality of our approach by providing a matching information-theoretic lower bound (up to a poly-logarithmic factor). Technically, we utilize the Random Utility Model (RUM) interpretation (outlined in Section 2.1) of the MNL model to prove both the upper bound and the fundamental limit, which could be of interest to analyzing more general class of RUMs.
Related work. In the context of collaborative ranking, MNL models have been proposed to model partial rankings from a pool of users. Existing work is limited to the case when each user provides pair-wise comparisons [1, 2]. [2] proposes solving a convex relaxation of maximizing the likelihood over matrices with bounded nuclear norm. It is shown that this approach achieves statistically optimal generalization error rate. Our analysis techniques are inspired by [1], which proposed the convex relaxation similar to ours, but when the users provide only pair-wise comparisons. For pairwise comparisons, our main result in Theorem 3 matches those of [1], but our result is more general in the sense that we analyze more general sampling models beyond pairwise comparisons. In general, “collaborative ranking” has been used typically to refer to the problem of learning personal rankings when the data is ratings on items (as opposed to ordinal data). Matrix factorization approaches have been widely applied in practice [3, 4], but no theoretical guarantees
are known. The remainder of the paper is organized as follows. In Section 2, we present the MNL model and propose a convex relaxation for learning the model, in the context of collaborative ranking. We provide theoretical guarantees for collaborative ranking in Section 3. In Section 4, we present the problem statement for bundled choice modeling, and analyze a similar convex relaxation approach. Notations. We use |||A|||Fand |||A|||∞to denote the Frobenius norm and the `∞ norm, |||A|||nuc =∑ i σi(A) to denote the nuclear norm where σi(A) denote the i-th singular value, and |||A|||2 = σ1(A)
for the spectral norm. We use 〈〈u, v〉〉 = ∑
i uivi and ‖u‖ to denote the inner product and the Euclidean norm. All ones vector is denoted by 1 and I(A) is the indicator function of the event A. The set of the fist N integers are denoted by [N ] = {1, . . . , N}.

2 Model and Algorithm: In this section, we present a discrete choice modeling for collaborative ranking, and propose an inference algorithm for learning the model from ordinal data.

2.1 MultiNomial Logit (MNL) model for comparative judgment: In collaborative ranking, we want to model how people who have similar preferences on a subset of items are likely to have similar tastes on other items as well. When users provide ratings, as in collaborative filtering applications, matrix factorization models are widely used since the low-rank structure captures the similarities between users. When users provide ordered preferences, we use a discrete choice model known as MultiNomial Logit (MNL) model that has a similar low-rank structure that captures the similarities between users and items.
Let Θ∗ be the d1×d2 dimensional matrix capturing the preference of d