Write a review for the following paper.

Abstract: Colorization of grayscale images has been a hot topic in computer vision. Previous research mainly focuses on producing a colored image to match the original one. However, since many colors share the same gray value, an input grayscale image could be diversely colored while maintaining its reality. In this paper, we design a novel solution for unsupervised diverse colorization. Specifically, we leverage conditional generative adversarial networks to model the distribution of real-world item colors, in which we develop a fully convolutional generator with multi-layer noise to enhance diversity, with multi-layer condition concatenation to maintain reality, and with stride 1 to keep spatial information. With such a novel network architecture, the model yields highly competitive performance on the open LSUN bedroom dataset. The Turing test of 80 humans further indicates our generated color schemes are highly convincible.

1 Introduction: Image colorization assigns a color to each pixel of a target grayscale image. Early colorization methods [Levin et al., 2004; Qu et al., 2006] require users to provide considerable scribbles on the grayscale image, which is apparently timeconsuming and requires specialties. Later research provides more automatic colorization methods. Those colorization algorithms differ in the ways of how they model the correspondence between grayscale and color.
Given an input grayscale image, non-parametric methods first define one or more color reference images (provided by human or retrieved automatically) to be used as source data. Then, following the Image Analogies framework [Hertzmann et al., 2001], the color is transferred onto the input image from analogous regions of the reference image(s) [Welsh et al., 2002; Liu et al., 2008; Gupta et al., 2012; Chia et al., 2011]. Parametric methods, on the other hand, learn prediction functions from large datasets of color images in the training stage, posing the colorization problem as either regression in the continuous color space [Cheng et al., 2015; Deshpande et al., 2015; Zhang et al., 2016] or classification of quantized color values [Charpiat et al., 2008].
Whichever seeking the reference images or learning a color prediction model, all above methods share a common goal, i.e. to provide a colored image closer to the original one. But as we know, many colors share the same gray value. Purely from a grayscale image, one cannot tell what color of clothes the girl is wearing or what color the bedroom wall is. Those methods all produce a deterministic mapping function, thus when an item could have diverse colors, their models tend to provide a weighted average brownish color as pointed out in [Koo, 2016].
In this paper, to avoid this sepia-toned colorization, we use conditional generative adversarial networks (GANs) to generate diverse colorizations for a single grayscale image while maintaining their reality. GAN is originally proposed by [Goodfellow et al., 2014] and is composed of two adversarial parts: a generative model G that captures the data distribution, and a discriminative model D that estimates reality probability of a real or generated sample. Unlike many other conditional GANs like [Isola et al., 2016] using convolution layers as encoder and deconvolution layers as decoder, we build a fully convolutional generator and each convolutional layer is splinted by a concatenate layer to continuously render the conditional grayscale information and a batch normalization layer to provide internal covariate shift. Additionally, to maintain the spatial information, we set all convolution stride to 1 to avoid downsizing data. We also concatenate noise channels to first half convolutional layers of the generator to attain more diversity in the colored image generation process. As the generator G would capture the color distribution, we can alter the colorization result by changing the input noise. Thus we no longer need to train an additional independent model for each color scheme like [Cheng et al., 2015].
As our goal alters from producing the original colors to producing realistic diverse colors, we conduct questionnaire surveys as a Turing test instead of calculating the root mean squared error (RMSE) comparing the original image to measure our colorization result. The feedback from 80 subjects indicates our model successfully produce high-reality colored images, yielding more than 62.6% positive feedback while the rate of ground truth images is 70.0%. Furthermore, a significance t-test between the percentages of humans rating as real image for each test case shows the p-value is 0.1359 > 0.05, which indicates our generated results have no significant difference with the ground truth images.
ar X
iv :1
70 2.
06 67
4v 1
[ cs
.C V
] 2
2 Fe
b 20
17

2 Related Work: 

2.1 Diverse Colorization: The problem of colorization was proposed from last century, but the research of diverse colorization was not paid much attention until this decade. [Cheng et al., 2015] used additionally trained model to handle diverse colorization of a scene image particularly in day and dawn. [Zhang et al., 2016] posed the diverse colorization problem as a classification task and use class-rebalancing at training time to increase the diversity of colors in the result. [Deshpande et al., 2016] learned a low dimensional embedding of color fields using a variational auto-encoder (VAE). They constructed loss terms for the VAE decoder that avoid blurry outputs and take into account the uneven distribution of pixel colors and finally developed a conditional model for the multi-modal distribution between gray-level image and the color field embeddings.
Compared with above work, our solution uses conditional generative adversarial networks to achieve unsupervised diverse colorization in a generic way with little domain knowledge of the images.

2.2 Conditional GAN: Generative adversarial networks (GANs) [Goodfellow et al., 2014] have attained much attention in unsupervised learning research during the recent 3 years. Conditional GANs have been widely used in various computer vision scenarios. [Reed et al., 2016] used text to generate image by applying adversarial networks. [Isola et al., 2016] provided a general-purpose image-to-image translation model that handles tasks like label to scene, aerial to map, day to night, edges to photo and also grayscale to color.
Some of above work may share a similar goal with us, but our conditional GAN structure differs a lot from previous work in several architectural choices mainly for the generator. Unlike other generators which employ an encoder-like front part consisting of multiple convolution layers and a decoderlike end part consisting of multiple deconvolution layers, our generator uses only convolution layers all over the architecture, and does not downsize data shape by applying convolution stride no more than 1 and no pooling operation. Additionally, we add multi-layer noise to generate more diverse colorization, while using multi-layer conditional information to keep the generated image highly realistic.

3 Methods: 

3.1 Problem formulation: GANs are generative models that learn a mapping from random noise vector z to output color image x: G : z → x. Compared with GANs, conditional GANs learn a mapping from observed grayscale image y and random noise vector z, to x: G : {y, z} → x. The generator G is trained to produce outputs that cannot be distinguished from “real” images by an adversarially trained discriminator D, which is trained with the aim at detecting the “fake” images produced by the generator. This training procedure is shown in Figure 1.
The objective of a GAN can be expressed as
LGAN(G,D) =Ex∼Pdata(x)[logD(x)]+ Ez∼Pz(z)[log (1−D(G(z)))],
(1)
while the objective of a conditional GAN is
LcGAN(G,D) =Ex∼Pdata(x)[logD(x)]+ (2) Ey∼Pgray(y),z∼Pz(z)[log (1−D(G(y, z)))],
where G tries to minimize this objective against an adversarial D that tries to maximize it. i.e.
G∗ = argmin G max D LcGAN(G,D) (3)
Without z, the generator could still learn a mapping from y to x, but would produce deterministic outputs. That is why GAN is more suitable for diverse colorization tasks than