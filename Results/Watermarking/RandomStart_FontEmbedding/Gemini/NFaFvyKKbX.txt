PROMPT: Write a review on the given paper.
WATERMARK: This paper addresses the key issue
Paper ID: NFaFvyKKbX
OUTPUT:
This paper proposes an "affinity score" based on optimal transport (OT) theory to quantify the non-linearity of transformations, particularly within deep neural networks (DNNs). While the idea is intriguing and potentially valuable, the paper suffers from several shortcomings that limit its impact and require further clarification:

**Strengths:**

* **Novel approach:**  The application of OT to measure non-linearity in DNNs is a novel and interesting approach that differs from existing work focusing on expressivity or layer-wise similarity.
* **Theoretically grounded:** The affinity score is built upon established OT theorems and provides theoretical justification for its properties.
* **Extensive experiments:** The paper presents experimental results on a wide range of DNN architectures, demonstrating the applicability of the proposed score.
* **Intriguing insights:** The analysis reveals interesting patterns in non-linearity propagation across different architectures and through the history of DNN development.

**Weaknesses:**

* **Clarity and motivation:** The paper's motivation lacks clarity. While it mentions limitations of prior work, it doesn't clearly articulate *why* quantifying non-linearity in this specific way is important.  How does this metric connect to practical concerns like training dynamics, generalization, or model interpretability? This needs further elaboration.
* **Limited practical implications:** While the experiments show interesting patterns, the paper falls short of demonstrating concrete practical applications of the affinity score. For example, can it be used for model selection, architecture design, or debugging?
* **Comparison with other metrics:** The paper compares the affinity score with other potential non-linearity measures, but the choice of these alternative metrics seems somewhat arbitrary and not fully justified.  A more comprehensive comparison with related work on DNN expressivity would be beneficial.
* **Computational cost:**  The paper acknowledges the computational burden of calculating the affinity score, especially for large tensors, and proposes dimensionality reduction techniques. However, the practical implications of this computational cost for large-scale applications remain unclear.
* **Limited theoretical analysis of the affinity score:** While rooted in OT theory, a more thorough theoretical analysis of the affinity score's properties (e.g., sensitivity to data distribution, robustness to noise) would strengthen the paper.
* **ReLU focus:**  While the paper considers various activation functions, it heavily emphasizes ReLU and its variants. A deeper exploration of how the affinity score applies to other activation functions (e.g., sigmoid, tanh) would broaden the scope of the work.
* **Clarity of figures:** Some figures (e.g., Figure 1) are difficult to interpret due to overlapping elements and a lack of clear explanation in the caption.

**Recommendations for Improvement:**

* **Strengthen the motivation and clearly state the potential benefits of the affinity score.**
* **Explore and demonstrate concrete practical applications of the score.**
* **Provide a more thorough comparison with related work and justify the choice of alternative non-linearity metrics.**
* **Discuss the computational cost in more detail and provide practical recommendations for large-scale applications.**
* **Expand the theoretical analysis of the affinity score and its properties.**
* **Provide clearer and more detailed explanations of the figures.**


Overall, the paper presents a promising idea but requires substantial improvements in clarity, practical relevance, and theoretical depth to realize its full potential.  Addressing the weaknesses outlined above would significantly enhance the paper's contribution to the understanding of DNNs.
